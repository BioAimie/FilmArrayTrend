ggplot(scored.df, aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + goem_point() + facet_wrap(~CustomerSiteId)
ggplot(scored.df, aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + facet_wrap(~CustomerSiteId)
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- readLines('../DataSources/CustomerSiteIdsWithNames.sql')
query <- paste(queryVector ,collapse="\n")
custnames.df <- sqlQuery(FADWcxn,query)
head(custnames.df)
odbcClose(FADWcxn)
head(custnames.df)
head(scored.df)
merge(scored.df, custnames.df[,c('CustomerSiteId','Name')], by='CustomerSiteId')
ggplot(merge(scored.df, custnames.df[,c('CustomerSiteId','Name')], by='CustomerSiteId'), aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + facet_wrap(~Name)
PMScxn <- odbcConnect('PMS_PROD')
queryVector <- readLines('../DataSources/SQL/EnteroD68/qcMedianCpRP.sql')
query <- paste(queryVector, collapse = '\n')
qc.lot.cps <- sqlQuery(PMScxn, query)
odbcClose(PMScxn)
head(qc.lot.cps)
head(qc.lot.cps)
ggplot(subset(qc.lot.cps, Name=='yeastRNA'), aes(x=MedianCp)) + geom_histogram()
head(cp.spread)
ggplot(cp.spread, aes(x=Date, y=yeastRNA)) + geom_point()
heaD(cp.norm)
head(cp.norm)
a <- merge(cp.spread, qc.lot.cps, by.x='LotNo', by.y='PouchLotNumber')
head(a)
a <- merge(cp.spread, subset(qc.lot.cps, Name=='yeastRNA'), by.x='LotNo', by.y='PouchLotNumber')
head(a)
ggplot(a, aes(x=Date, y=yeastRNA)) + geom_point(color='black') + geom_point(data=a, aes(x=Date, y=MedianCp), color='blue', alpha=0.2)
mean(subset(qc.lot.cps, Name=='yeastRNA')$MedianCp)
qc.yeast.cp.avg <- mean(subset(qc.lot.cps, Name=='yeastRNA')$MedianCp)
head(merge(data.frame(cp.spread[,1:4], cp.spread[,5:10]/cp.spread$yeastRNA), qc.lot.cps[qc.lot.cps$Name=='yeastRNA',c('PouchLotNumber','MedianCp')], by.x='LotNo', by.y='PouchLotNumber'))
head(cp.spread)
qc.yeast.cp.avg <- mean(subset(qc.lot.cps, Name=='yeastRNA')$MedianCp)
a <- merge(cp.spread, subset(qc.lot.cps, Name=='yeastRNA'), by.x='LotNo', by.y='PouchLotNumber')
head(a)
head(qc.yeast.cp.avg)
a$HRV1/a$yeastRNA*a$MedianCp/qc.yeast.cp.avg
a$TryOneHRV1 <- a$HRV1/a$yeastRNA*a$MedianCp/qc.yeast.cp.avg
heaD(a)
head(a)
ggplot(a, aes(x=Date, y=HRV1)) + geom_point()
ggplot(a, aes(x=Date, y=HRV1)) + geom_point() + geom_point(data=a, aes(x=Date, y=TryOneHRV1), color='blue', alpha=0.2)
ggplot(a, aes(x=Date, y=HRV1)) + geom_point() + geom_point(data=a, aes(x=Date, y=10*TryOneHRV1), color='blue', alpha=0.2)
ggplot(a, aes(x=Date, y=HRV1)) + geom_point() + geom_point(data=a, aes(x=Date, y=10*TryOneHRV1), color='blue', alpha=0.1)
ggplot(a, aes(x=Date, y=yeastRNA)) + geom_point() + geom_point(data=a, aes(x=Date, y=10*TryOneHRV1), color='blue', alpha=0.1)
ggplot(a, aes(x=Date, y=HRV1/yeastRNA)) + geom_point() + geom_point(data=a, aes(x=Date, y=10*TryOneHRV1), color='blue', alpha=0.1)
ggplot(a, aes(x=Date, y=HRV1/yeastRNA)) + geom_point() + geom_point(data=a, aes(x=Date, y=TryOneHRV1), color='blue', alpha=0.1)
ggplot(a, aes(x=Date, y=HRV1/yeastRNA)) + geom_point() + geom_point(data=a, aes(x=Date, y=TryOneHRV1), color='blue', alpha=0.1) + geom_point(a, aes(x=Date, y=yeastRNA/30), color='red', alpha=0.1)
ggplot(a, aes(x=Date, y=HRV1/yeastRNA)) + geom_point() + geom_point(data=a, aes(x=Date, y=TryOneHRV1), color='blue', alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/30), color='red', alpha=0.1)
ggplot(a, aes(x=Date, y=HRV1/yeastRNA)) + geom_point() + geom_point(data=a, aes(x=Date, y=5*TryOneHRV1), color='blue', alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/35), color='red', alpha=0.1)
ggplot(a, aes(x=Date, y=HRV1/yeastRNA)) + geom_point() + geom_point(data=a, aes(x=Date, y=2*TryOneHRV1), color='blue', alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/35), color='red', alpha=0.1)
ggplot(a, aes(x=Date, y=HRV1/yeastRNA)) + geom_point() + geom_point(data=a, aes(x=Date, y=2*TryOneHRV1), color='blue', alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/35), color='red', alpha=0.1) + scale_color_manual(values=c('black','blue','red'))
ggplot(a, aes(x=Date, y=HRV1/yeastRNA, color='RunNorm')) + geom_point() + geom_point(data=a, aes(x=Date, y=2*TryOneHRV1, color='YeastNormRunNorm'), color='blue', alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/35, color='YeastRaw'), color='red', alpha=0.1) + scale_color_manual(values=c('black','blue','red'))
ggplot(a, aes(x=Date, y=HRV1/yeastRNA, color='RunNorm')) + geom_point() + geom_point(data=a, aes(x=Date, y=2*TryOneHRV1, color='YeastNormRunNorm'), alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/35, color='YeastRaw'), alpha=0.1) + scale_color_manual(values=c('black','blue','red'))
ggplot(a, aes(x=Date, y=HRV1/yeastRNA, color='RunNorm (Cp/CpYeast)')) + geom_point() + geom_point(data=a, aes(x=Date, y=2*TryOneHRV1, color='YeastNormRunNorm (Cp/CpYeast*CpYeastQC/CpYeastQCAvg)'), alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/35, color='YeastRaw (CpYeast/35)'), alpha=0.1) + scale_color_manual(values=c('black','blue','red'))
ggplot(a, aes(x=Date, y=HRV1/yeastRNA, color='RunNorm (Cp/CpYeast)')) + geom_point() + geom_point(data=a, aes(x=Date, y=1.5*TryOneHRV1, color='YeastNormRunNorm (Cp/CpYeast*CpYeastQC/CpYeastQCAvg)'), alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/35, color='YeastRaw (CpYeast/35)'), alpha=0.1) + scale_color_manual(values=c('black','blue','red'))
head(qc.lot.cps)
head(tm.sparse.handler)
head(tm.spread)
ggplot(tm.spread, aes(x=Date, y=yeastRNA)) + geom_point()
setwd('~/FilmArrayTrend/EnteroD68/')
# load the neccessary libraries
library(RODBC)
library(lubridate)
library(ggplot2)
library(mgcv)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
library(rgl)
library(AnomalyDetection)
# Set up variables needed later in the analysis
# ===========================================================================================
# create an Epi date calendar that will be used by all the data sets
startYear <- 2013
calendar.df <- createCalendarLikeMicrosoft(startYear, 'Week')
calendar.df <- transformToEpiWeeks(calendar.df)
calendar.df$YearWeek <- with(calendar.df, ifelse(Week < 10, paste(Year, Week, sep='-0'), paste(Year, Week, sep='-')))
calendar.df$Days <- 1
# set up some constants
imgDir <- 'Figures/'
dateBreaks <- unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])[order(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek']))][seq(1, length(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])), 8)]
# Load in the data
# ===========================================================================================
# set some query variables, like the customer site... also, get the number of RP runs by site
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- readLines('../DataSources/CustomerSiteIdsWithNames.sql')
query <- paste(queryVector ,collapse="\n")
custnames.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/sitesRunningRP.txt',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
sites.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/rpRunsBySite.sql',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
runs.df <- sqlQuery(FADWcxn,query)
odbcClose(FADWcxn)
PMScxn <- odbcConnect('PMS_PROD')
queryVector <- readLines('../DataSources/SQL/EnteroD68/qcMedianCpRP.sql')
query <- paste(queryVector, collapse = '\n')
qc.lot.cps <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/qcMedianTmRP.sql')
query <- paste(queryVector, collapse = '\n')
qc.lot.tms <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/rhinoDataAtCHLA.sql')
query <- paste(queryVector, collapse = '\n')
chla.df <- sqlQuery(PMScxn, query)
odbcClose(PMScxn)
dat.df <- c()
choose.sites <- as.character(sites.df[,'CustomerSiteId'])
for(j in 1:length(choose.sites)) {
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- readLines('../DataSources/SQL/EnteroD68/rhinoDataBySite.sql')
query <- paste(gsub('SITE_INDEX', choose.sites[j], queryVector), collapse="\n")
dat.site.df <- sqlQuery(FADWcxn, query)
odbcClose(FADWcxn)
dat.df <- rbind(dat.df, dat.site.df)
}
rm(dat.site.df)
# Clean the data
# ============================================================================================
# with the data, determine the median value of Tm, Cp, and Max Fluor for each assay in the HRV/EV target
cp.median <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=dat.df)
# cp.median.chla <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=chla.df)
# cp.median.chla$RunDataId <- seq((max(dat.df$RunDataId)+1), (max(dat.df$RunDataId) + length(cp.median.chla$Cp)), 1)
# cp.median <- rbind(cp.median, cp.median.chla)
cp.spread <- spread(data = cp.median, key = AssayName, value = Cp)
cp.sparse.handler <- 40
tm.median <- aggregate(Tm~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=dat.df)
# tm.median.chla <- aggregate(Tm~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=chla.df)
# tm.median.chla$RunDataId <- seq((max(dat.df$RunDataId)+1), (max(dat.df$RunDataId) + length(tm.median.chla$Tm)), 1)
# tm.median <- rbind(tm.median, tm.median.chla)
tm.spread <- spread(data = tm.median, key = AssayName, value = Tm)
tm.sparse.handler <- 5
head(cp.median)
head(cp.spread)
head(cp.spread)
head(tm.spread)
qc.yeast.cp.avg <- mean(subset(qc.lot.cps, Name=='yeastRNA')$MedianCp)
head(cp.spread)
qc.yeast.cp.avg <- mean(subset(qc.lot.cps, Name=='yeastRNA')$MedianCp)
cp.norm <- merge(data.frame(cp.spread[,1:4], cp.spread[,5:10]/cp.spread$yeastRNA), qc.lot.cps[qc.lot.cps$Name=='yeastRNA',c('PouchLotNumber','MedianCp')], by.x='LotNo', by.y='PouchLotNumber')
cp.norm <- data.frame(cp.norm[,1:4], cp.norm[,5:10]*cp.norm$MedianCp/qc.yeast.cp.avg)
colnames(cp.norm)[5:10] <- paste(colnames(cp.norm[5:10]), 'Cp', sep='_')
head(cp.norm)
hist(cp.norm$HRV1_Cp)
hist(cp.norm$HRV2_Cp)
hist(cp.norm$HRV3_Cp)
hist(cp.norm$HRV4_Cp)
hist(cp.norm$Entero1_Cp)
hist(cp.norm$Entero2_Cp)
cp.sparse.handler <- 5 # TRY TUNING THIS????
cp.norm[,c(5:10)][is.na(cp.norm[,c(5:10)])] <- cp.sparse.handler
qc.yeast.tm.avg <- mean(subset(qc.lot.tms, Name=='yeastRNA')$MedianTm)
qc.yeast.tm.avg <- mean(subset(qc.lot.tms, Name=='yeastRNA')$MedianTm)
tm.norm <- merge(data.frame(tm.spread[,1:4], tm.spread[,5:10]/tm.spread$yeastRNA), qc.lot.tms[qc.lot.tms$Name=='yeastRNA',c('PouchLotNumber','MedianTm')], by.x='LotNo', by.y='PouchLotNumber')
head(tm.norm)
tm.norm <- data.frame(RunDataId = tm.spread$RunDataId, tm.spread[,5:10]*tm.norm$MedianTm/qc.yeast.tm.avg)
colnames(tm.norm)[2:7] <- paste(colnames(tm.norm[2:7]), 'Tm', sep='_')
head(tm.norm)
hist(tm.norm$HRV1_Tm)
hist(tm.norm$HRV2_Tm)
qc.yeast.tm.avg
head(tm.spread)
hist(tm.norm$HRV3_Tm)
hist(tm.norm$HRV4_Tm)
hist(tm.norm$Entero1_Tm)
hist(tm.norm$Entero2_Tm)
max(tm.norm$Entero2_Tm)
max(tm.norm$Entero2_Tm, na.rm=TRUE)
max(tm.norm$Entero1_Tm, na.rm=TRUE)
max(tm.norm$HRV1_Tm, na.rm=TRUE)
max(tm.norm$HRV2_Tm, na.rm=TRUE)
max(tm.norm$HRV3_Tm, na.rm=TRUE)
max(tm.norm$HRV4_Tm, na.rm=TRUE)
tm.sparse.handler <- 100 # TRY TUNING THIS????
tm.norm[,c(2:7)][is.na(tm.norm[,c(2:7)])] <- tm.sparse.handler
dat.norm <- merge(cp.norm, tm.norm, by='RunDataId')
head(dat.norm)
head(dat.norm)
site.rhino.count <- with(merge(data.frame(cp.spread, Positive=1), calendar.df, by='Date'), aggregate(Positive~YearWeek+CustomerSiteId, FUN=sum))
sites <- as.character(unique(site.rhino.count$CustomerSiteId))[order(as.character(unique(site.rhino.count$CustomerSiteId)))]
site.rhino.count <- do.call(rbind, lapply(1:length(sites), function(x) data.frame(merge(data.frame(YearWeek = unique(calendar.df[,c('YearWeek')]), CustomerSiteId = sites[x]), site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], c('YearWeek','Positive')], by='YearWeek', all.x=TRUE))))
site.rhino.count[is.na(site.rhino.count$Positive), 'Gap'] <- 1
site.rhino.count[is.na(site.rhino.count$Gap), 'Gap'] <- 0
periods <- unique(as.character(site.rhino.count$YearWeek))
site.gaps <- do.call(rbind, lapply(1:length(sites), function(x) do.call(rbind, lapply(5:length(periods), function(y) data.frame(YearWeek = periods[y], CustomerSiteId = sites[x], MissingPeriods = sum(site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], 'Gap'][(y-4):y]))))))
site.starts <- do.call(rbind, lapply(1:length(sites), function(x) site.gaps[site.gaps$CustomerSiteId==sites[x],][max(which(site.gaps[site.gaps$CustomerSiteId==sites[x], 'MissingPeriods']==5)), c('CustomerSiteId','YearWeek')]))
dat.features <- dat.norm[,2:16]
dat.features <- merge(dat.features, calendar.df[,c('Date','YearWeek')], by='Date')
dat.trim <- do.call(rbind, lapply(1:length(site.starts$CustomerSiteId), function(x) filter(dat.features, (CustomerSiteId == site.starts[x,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[x,'YearWeek'])))))
initial.window <- 100
test.horizon <- 10
sites
sites[order(as.numeric(sites))]
sites.all <- sites
sites
sites[c(2, 5, 6, 9, 17)]
sites[c(2, 5, 6, 9, 18)]
sites[c(2, 5, 6, 10, 18)]
sites <- sites.all[c(2, 5, 6, 10, 18)]
i <- 1
site.features <- filter(dat.trim, CustomerSiteId == sites[i])
site.features <- site.features[with(site.features, order(Date)), ]
nrow(site.features)==0
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))]
head(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
head(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
site.df <- c()
# site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
# split into train and test data
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
if(FALSE)  {
# get the near zero variance of the train set so that these variables can be removed from the analysis???
# NOT SURE IF I WANT TO KEEP THIS BECAUSE WHAT IF THE VARIABLE DOES HAVE VARIANCE IN THE TEST SET... AM I GETTING RID OF THAT???
# train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
# train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
# # agg.nzv <- nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
# # agg.remove.vars <- row.names(agg.nzv[agg.nzv$nzv==TRUE,])
# # nzv_score <- ifelse(length(agg.remove.vars) > length(train.remove.vars), 1, 0)
# site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
# site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
}
# transform the data... center, scale, and use Box-Cox to transform the data using PCA or Box-Cox
#### I DON'T KNOW IF THIS IS NECCESSARY AFTER NORMALIZING Cp AND Tm BECAUSE THE DISTRIBUTIONS ARE SUPER NORMAL
# pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
# site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
# site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
# site.train.trans <- predict(bc.trans, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
# site.test.trans <- predict(bc.trans, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
site.train.trans <- site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))]
site.test.trans <- site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.1
guess.mpt <- 90
eps.interval <- 0.1
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
# iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
# print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.trans$Cluster <- as.factor(guess.res$cluster)
site.test.trans$Cluster <- unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
# count the number of clusters in the test set that are considered noise
train.noise <- nrow(site.train.trans[site.train.trans$Cluster==0, ])
test.noise <- nrow(site.test.trans[site.test.trans$Cluster==0, ])
# figure out the number of Principle Components needed to explain at least 95% of the variance for the train set and then the
# train + test set...
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train.var <- 1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
pca.train.count <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test.var <- 1-sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))[min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))]
pca.test.count <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
pca.test.var.at.train.pca <- 1 - pca.test$sdev[pca.train.count]^2/sum(pca.test$sdev^2)
# create some data frame that contains information about the timeslice
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TestStartDate = site.test[site.test$Obs==min(site.test$Obs), 'Date'],
TrainNoise = train.noise, TestNoise = test.noise, # RatioNoise = test.horizon*test.noise/train.noise,
TrainPCA = pca.train.count, TrainVar = pca.train.var, TestPCA = pca.test.count, TestVar = pca.test.var,
TestVarWithTrainPCA = pca.test.var.at.train.pca) #,
# RatioPCA = pca.test.count/pca.train.count)
site.df <- rbind(site.df, temp)
}
head(site.df)
head(site.df)
ggplot(site.df, aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + scale_color_gradient(low='red')
ggplot(site.df, aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + scale_color_gradient(low='red')
ggplot(site.df, aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + scale_color_gradient(low='red')
dev.off()
ggplot(site.df, aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + scale_color_gradient(low='red')
sites[i]
scored.df <- rbind(scored.df, site.df)
for (i in 2:length(sites)) {
# parition the data by site and then order by the test date
site.features <- filter(dat.trim, CustomerSiteId == sites[i])
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { next }
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
# site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
# split into train and test data
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
if(FALSE)  {
# get the near zero variance of the train set so that these variables can be removed from the analysis???
# NOT SURE IF I WANT TO KEEP THIS BECAUSE WHAT IF THE VARIABLE DOES HAVE VARIANCE IN THE TEST SET... AM I GETTING RID OF THAT???
# train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
# train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
# # agg.nzv <- nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
# # agg.remove.vars <- row.names(agg.nzv[agg.nzv$nzv==TRUE,])
# # nzv_score <- ifelse(length(agg.remove.vars) > length(train.remove.vars), 1, 0)
# site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
# site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
}
# transform the data... center, scale, and use Box-Cox to transform the data using PCA or Box-Cox
#### I DON'T KNOW IF THIS IS NECCESSARY AFTER NORMALIZING Cp AND Tm BECAUSE THE DISTRIBUTIONS ARE SUPER NORMAL
# pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
# site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
# site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
# site.train.trans <- predict(bc.trans, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
# site.test.trans <- predict(bc.trans, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
site.train.trans <- site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))]
site.test.trans <- site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.1
guess.mpt <- 90
eps.interval <- 0.1
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
# iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
# print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.trans$Cluster <- as.factor(guess.res$cluster)
site.test.trans$Cluster <- unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
# count the number of clusters in the test set that are considered noise
train.noise <- nrow(site.train.trans[site.train.trans$Cluster==0, ])
test.noise <- nrow(site.test.trans[site.test.trans$Cluster==0, ])
# figure out the number of Principle Components needed to explain at least 95% of the variance for the train set and then the
# train + test set...
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train.var <- 1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
pca.train.count <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test.var <- 1-sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))[min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))]
pca.test.count <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
pca.test.var.at.train.pca <- 1 - pca.test$sdev[pca.train.count]^2/sum(pca.test$sdev^2)
# create some data frame that contains information about the timeslice
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TestStartDate = site.test[site.test$Obs==min(site.test$Obs), 'Date'],
TrainNoise = train.noise, TestNoise = test.noise, # RatioNoise = test.horizon*test.noise/train.noise,
TrainPCA = pca.train.count, TrainVar = pca.train.var, TestPCA = pca.test.count, TestVar = pca.test.var,
TestVarWithTrainPCA = pca.test.var.at.train.pca) #,
# RatioPCA = pca.test.count/pca.train.count)
site.df <- rbind(site.df, temp)
}
# print(Sys.time() - site.start.time)
scored.df <- rbind(scored.df, site.df)
}
head(scored.df)
ggplot(scored.df, aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + scale_color_gradient(low='red') + facet_wrap(~CustomerSiteId)
ggplot(merge(scored.df, custnames.df, by='CustomerSiteId'), aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + scale_color_gradient(low='red') + facet_wrap(~Name)
sites
i
head(scored.df)
unique(scored.df$CustomerSiteId)
unique(as.character(scored.df$CustomerSiteId))
head(scored.df)
tail(scored.df)
a <- scored.df[scored.df$CustomerSiteId=='13', ]
View(a)
16548/2
head(a[8274:nrow(a), ])
head(a[8174:nrow(a), ])
head(a[8194:nrow(a), ])
head(a[8254:nrow(a), ])
head(a[8244:nrow(a), ])
head(a[8234:nrow(a), ])
head(a[8224:nrow(a), ])
head(a[8220:nrow(a), ])
head(a[8218:nrow(a), ])
head(a[8219:nrow(a), ])
sites <- sites.all[c(2, 5, 6, 10, 18)]
scored.df <- c()
for (i in 2:length(sites)) {
# parition the data by site and then order by the test date
site.features <- filter(dat.trim, CustomerSiteId == sites[i])
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { next }
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
# site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
# split into train and test data
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
if(FALSE)  {
# get the near zero variance of the train set so that these variables can be removed from the analysis???
# NOT SURE IF I WANT TO KEEP THIS BECAUSE WHAT IF THE VARIABLE DOES HAVE VARIANCE IN THE TEST SET... AM I GETTING RID OF THAT???
# train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
# train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
# # agg.nzv <- nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
# # agg.remove.vars <- row.names(agg.nzv[agg.nzv$nzv==TRUE,])
# # nzv_score <- ifelse(length(agg.remove.vars) > length(train.remove.vars), 1, 0)
# site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
# site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
}
# transform the data... center, scale, and use Box-Cox to transform the data using PCA or Box-Cox
#### I DON'T KNOW IF THIS IS NECCESSARY AFTER NORMALIZING Cp AND Tm BECAUSE THE DISTRIBUTIONS ARE SUPER NORMAL
# pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
# site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
# site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
site.train.trans <- predict(bc.trans, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
site.test.trans <- predict(bc.trans, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
# site.train.trans <- site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))]
# site.test.trans <- site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.1
guess.mpt <- 90
eps.interval <- 0.1
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
# iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
# print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.trans$Cluster <- as.factor(guess.res$cluster)
site.test.trans$Cluster <- unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
# count the number of clusters in the test set that are considered noise
train.noise <- nrow(site.train.trans[site.train.trans$Cluster==0, ])
test.noise <- nrow(site.test.trans[site.test.trans$Cluster==0, ])
# figure out the number of Principle Components needed to explain at least 95% of the variance for the train set and then the
# train + test set...
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train.var <- 1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
pca.train.count <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test.var <- 1-sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))[min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))]
pca.test.count <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
pca.test.var.at.train.pca <- 1 - pca.test$sdev[pca.train.count]^2/sum(pca.test$sdev^2)
# create some data frame that contains information about the timeslice
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TestStartDate = site.test[site.test$Obs==min(site.test$Obs), 'Date'],
TrainNoise = train.noise, TestNoise = test.noise, # RatioNoise = test.horizon*test.noise/train.noise,
TrainPCA = pca.train.count, TrainVar = pca.train.var, TestPCA = pca.test.count, TestVar = pca.test.var,
TestVarWithTrainPCA = pca.test.var.at.train.pca) #,
# RatioPCA = pca.test.count/pca.train.count)
site.df <- rbind(site.df, temp)
}
# print(Sys.time() - site.start.time)
scored.df <- rbind(scored.df, site.df)
}
head(scored.df)
ggplot(merge(scored.df, custnames.df, by='CustomerSiteId'), aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + scale_color_gradient(low='red') + facet_wrap(~Name)
XVals<-seq(0,5*pi,0.05)
SinVals<-sin(XVals)
Noise<-rnorm(length(XVals),0,runif(length(XVals),0.25,1))
RawData<-4+SinVals+Noise
MinFunc<-function(param)
{
diff<-RawData-(param+SinVals)
diffsq<-diff^2
sum(diffsq)
}
Correction<-optimize(MinFunc,c(-20,20)) #might need to adjust the second argument to cover a larger range
plot(XVals,RawData,type="l")
lines(XVals,SinVals+Correction$minimum,col="red",lwd=2)
library(RODBC)
library(lubridate)
library(ggplot2)
library(mgcv)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
library(rgl)
library(AnomalyDetection)
head(scored.df)
ggplot(subset(scored.df, CustomerSiteId=='13'), aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise)) + geom_point()
ggplot(subset(scored.df, CustomerSiteId=='26'), aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise)) + geom_point()
ggplot(merge(scored.df, custnames.df[,c('CustomerSiteId','Name')], by='CustomerSiteId'), aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + facet_wrap(~Name)
custnames.df
ggplot(subset(scored.df, CustomerSiteId=='26'), aes(x=TestStartDate, y=test.horizon*TestNoise/TrainNoise)) + geom_point()
head(cp.median)
head(cp.spread)
ggplot(subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=yeastRNA)) + geom_point()
ggplot(subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=yeastRNA)) + geom_point() + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero1), color='dodgerblue', alpha=0.2)
ggplot(subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=yeastRNA)) + geom_point() + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero1), color='dodgerblue', alpha=0.3)
ggplot(subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=yeastRNA)) + geom_point(color='black', alpha=0) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero1), color='dodgerblue', alpha=0.3)
ggplot(subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=yeastRNA)) + geom_point(color='black', alpha=0.5) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero1), color='dodgerblue', alpha=0.3)
ggplot(subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=yeastRNA)) + geom_point(color='black', alpha=0.5) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero1), color='dodgerblue', alpha=0.3) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero2), color='blue', alpha=0.3)
ggplot(subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=yeastRNA)) + geom_point(color='black', alpha=0.5) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero1), color='dodgerblue', alpha=0.3) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero2), color='blue', alpha=0.3) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=HRV1), color='maroon4', alpha=0.3)
ggplot(subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=yeastRNA)) + geom_point(color='black', alpha=0.5) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero1), color='dodgerblue', alpha=0.3) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=Entero2), color='blue', alpha=0.3) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=HRV1), color='maroon4', alpha=0.3) + geom_point(data = subset(cp.spread, CustomerSiteId=='26'), aes(x=Date, y=HRV2), color='maroon2', alpha=0.3)
head(cp.medain)
head(cp.median)
ggplot(subset(cp.median, CustomerSiteId=='26'), aes(x=Date, y=Cp)) + geom_point() + facet_wrap(~AssayName)
head(cp.norm)
rbind(data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'Entero1', CpNorm = subset(cp.norm, CustomerSiteId=='26')$Entero1_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'Entero2', CpNorm = subset(cp.norm, CustomerSiteId=='26')$Entero2_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'HRV1', CpNorm = subset(cp.norm, CustomerSiteId=='26')$HRV1_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'HRV2', CpNorm = subset(cp.norm, CustomerSiteId=='26')$HRV2_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'HRV3', CpNorm = subset(cp.norm, CustomerSiteId=='26')$HRV3_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'HRV4', CpNorm = subset(cp.norm, CustomerSiteId=='26')$HRV4_Cp))
a <- rbind(data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'Entero1', CpNorm = subset(cp.norm, CustomerSiteId=='26')$Entero1_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'Entero2', CpNorm = subset(cp.norm, CustomerSiteId=='26')$Entero2_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'HRV1', CpNorm = subset(cp.norm, CustomerSiteId=='26')$HRV1_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'HRV2', CpNorm = subset(cp.norm, CustomerSiteId=='26')$HRV2_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'HRV3', CpNorm = subset(cp.norm, CustomerSiteId=='26')$HRV3_Cp), data.frame(Date = subset(cp.norm, CustomerSiteId=='26')$Date, AssayName = 'HRV4', CpNorm = subset(cp.norm, CustomerSiteId=='26')$HRV4_Cp))
head(a)
ggplot(a, aes(x=Date, y=CpNorm)) + geom_point() + facet_wrap(~AssayName)
