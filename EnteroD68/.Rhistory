# iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
# print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.trans$Cluster <- as.factor(guess.res$cluster)
# site.test.trans$Cluster <- unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
site.test.trans$Cluster <- unname(predict(guess.res, site.train.trans[, grep('PC', colnames(site.train.trans))], site.test.trans))
# count the number of clusters in the test set that are considered noise
train.noise <- nrow(site.train.trans[site.train.trans$Cluster==0, ])
test.noise <- nrow(site.test.trans[site.test.trans$Cluster==0, ])
# figure out the number of Principle Components needed to explain at least 95% of the variance for the train set and then the
# train + test set...
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train.var <- 1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
pca.train.count <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test.var <- 1-sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))[min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))]
pca.test.count <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
pca.test.var.at.train.pca <- 1 - pca.test$sdev[pca.train.count]^2/sum(pca.test$sdev^2)
# create some data frame that contains information about the timeslice
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TestStartDate = site.test[site.test$Obs==min(site.test$Obs), 'Date'],
TrainNoise = train.noise, TestNoise = test.noise, # RatioNoise = test.horizon*test.noise/train.noise,
TrainPCA = pca.train.count, TrainVar = pca.train.var, TestPCA = pca.test.count, TestVar = pca.test.var,
TestVarWithTrainPCA = pca.test.var.at.train.pca) #,
# RatioPCA = pca.test.count/pca.train.count)
site.df <- rbind(site.df, temp)
}
# print(Sys.time() - site.start.time)
scored.df <- rbind(scored.df, site.df)
}
head(scored.df)
ggplot(scored.df, aes(x=TestStartDate, y=TestNoise/TrainNoise, color=TestVarWithTrainPCA)) + geom_point() + facet_wrap(~CustomerSiteId)
min(scored.df$Seq)
head(dat.trim)
head(dat.trim)
head(site.df)
head(scored.df)
head(site.train)
head(site.features)
head(site.features)
head(site.test)
head(site.features)
head(dat.trim)
head(dat.features)
head(dat.norm)
head(dat.norm)
head(site.features)
mark.evd68 <- read.csv('../DataSources/SQL/EnteroD68/ChilderensMercy26_SequencedEVD68.csv', header=TRUE)
head(mark.evd68)
head(dat.df)
head(mark.evd68)
head(dat.norm)
dat.features <- merge(dat.norm, calendar.df[,c('Date','YearWeek')], by='Date')
dat.trim <- do.call(rbind, lapply(1:length(site.starts$CustomerSiteId), function(x) filter(dat.features, (CustomerSiteId == site.starts[x,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[x,'YearWeek'])))))
site.features <- filter(dat.trim, CustomerSiteId == sites[i])
head(site.features)
site.features <- site.features[with(site.features, order(Date)), ]
site.features$Obs <- seq(1, length(site.features$Date), 1)
head(site.features)
j
j <- 101
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
head(site.test)
site.test
site.test[1,'RunDataId']
seq.id <- site.test[1,'RunDataId']
train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
head(site.train)
head(site.test)
pca.transform <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method = c('BoxCox','center','scale','pca'))
site.train.trans <- predict(pca.transform, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
site.test.trans <- predict(pca.transform, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
head(site.test.trans)
head(dat.df)
setwd('~/FilmArrayTrend/EnteroD68/')
# load the neccessary libraries
library(RODBC)
library(lubridate)
library(ggplot2)
library(mgcv)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
library(rgl)
library(AnomalyDetection)
# Set up variables needed later in the analysis
# ===========================================================================================
# create an Epi date calendar that will be used by all the data sets
startYear <- 2013
calendar.df <- createCalendarLikeMicrosoft(startYear, 'Week')
calendar.df <- transformToEpiWeeks(calendar.df)
calendar.df$YearWeek <- with(calendar.df, ifelse(Week < 10, paste(Year, Week, sep='-0'), paste(Year, Week, sep='-')))
calendar.df$Days <- 1
# set up some constants
imgDir <- 'Figures/'
dateBreaks <- unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])[order(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek']))][seq(1, length(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])), 8)]
# Load in the data
# ===========================================================================================
# set some query variables, like the customer site... also, get the number of RP runs by site
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- readLines('../DataSources/CustomerSiteIdsWithNames.sql')
query <- paste(queryVector ,collapse="\n")
custnames.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/sitesRunningRP.txt',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
sites.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/rpRunsBySite.sql',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
runs.df <- sqlQuery(FADWcxn,query)
odbcClose(FADWcxn)
PMScxn <- odbcConnect('PMS_PROD')
queryVector <- readLines('../DataSources/SQL/EnteroD68/qcMedianCpRP.sql')
query <- paste(queryVector, collapse = '\n')
qc.lot.cps <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/qcMedianTmRP.sql')
query <- paste(queryVector, collapse = '\n')
qc.lot.tms <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/rhinoDataAtCHLA.sql')
query <- paste(queryVector, collapse = '\n')
chla.df <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/CMH_SequencedRuns.sql')
query <- paste(queryVector, collapse = '\n')
cmh.df <- sqlQuery(PMScxn, query)
odbcClose(PMScxn)
# start a loop to gather Cp, Tm, and MaxFluor data for all HRV/Entero Assays by site
dat.df <- c()
choose.sites <- as.character(sites.df[,'CustomerSiteId'])
for(j in 1:length(choose.sites)) {
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- readLines('../DataSources/SQL/EnteroD68/rhinoDataBySite.sql')
query <- paste(gsub('SITE_INDEX', choose.sites[j], queryVector), collapse="\n")
dat.site.df <- sqlQuery(FADWcxn, query)
odbcClose(FADWcxn)
dat.df <- rbind(dat.df, dat.site.df)
}
rm(dat.site.df)
head(cmh.df)
head(chla.df)
head(dat.df[dat.df$CustomerSiteId==25, ])
dat.df[grep('72572', dat.df$PouchSerialNumber), ]
dat.df[grep('72572', dat.df$PouchSerialNumber), ]
dat.df[grep('735772', dat.df$PouchSerialNumber), ]
dat.df[grep('73577', dat.df$PouchSerialNumber), ]
dat.df[grep('7357', dat.df$PouchSerialNumber), ]
head(dat.df)
unique(dat.df$AssayName)
head(cmh.df)
head(chla.df)
PMScxn <- odbcConnect('PMS_PROD')
queryVector <- readLines('../DataSources/SQL/EnteroD68/rhinoDataAtCHLA.sql')
query <- paste(queryVector, collapse = '\n')
chla.df <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/CMH_SequencedRuns.sql')
query <- paste(queryVector, collapse = '\n')
cmh.df <- sqlQuery(PMScxn, query)
odbcClose(PMScxn)
head(chla.df)
head(cmh.df)
cp.median.chla <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=chla.df)
cp.median.cmh <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=cmh.df)
head(cp.median.chla)
head(cp.median.cmh)
head(cp.median)
cp.median <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=dat.df)
cp.median <- rbind(rbind(cp.median, cp.median.chla), cp.median.cmh)
spread(data=cp.median.cmh, key = AssayName, value = Cp)
a <- spread(data=cp.median.cmh, key = AssayName, value = Cp)
a <- spread(data=cp.median.chla, key = AssayName, value = Cp)
rm(a)
cp.median <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=dat.df)
cp.median <- rbind(rbind(cp.median, cp.median.chla), cp.median.cmh)
cp.spread <- spread(data = cp.median, key = AssayName, value = Cp)
tm.median <- aggregate(Tm~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=dat.df)
tm.median.chla <- aggregate(Tm~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=chla.df)
tm.median.cmh <- aggregate(Tm~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=cmh.df)
tm.median <- rbind(rbind(tm.median, tm.median.chla), tm.median.cmh)
tm.spread <- spread(data = tm.median, key = AssayName, value = Tm)
qc.yeast.cp.avg <- mean(subset(qc.lot.cps, Name=='yeastRNA')$MedianCp)
cp.norm <- merge(data.frame(cp.spread[,1:4], cp.spread[,5:10]/cp.spread$yeastRNA), qc.lot.cps[qc.lot.cps$Name=='yeastRNA',c('PouchLotNumber','MedianCp')], by.x='LotNo', by.y='PouchLotNumber')
cp.norm <- data.frame(cp.norm[,1:4], cp.norm[,5:10]*cp.norm$MedianCp/qc.yeast.cp.avg)
colnames(cp.norm)[5:10] <- paste(colnames(cp.norm[5:10]), 'Cp', sep='_')
head(cp.norm)
max(cp.norm[,5:10])
max(cp.norm[,5:10], na.rm=TRUE)
max(cp.norm[,5:11], na.rm=TRUE)
max(cp.norm[,5:10], na.rm=TRUE)
max(cp.norm[,5:10], na.rm=TRUE) + 1
cp.sparse.handler <- 3 # TRY TUNING THIS????
cp.norm[,c(5:10)][is.na(cp.norm[,c(5:10)])] <- cp.sparse.handler
qc.yeast.tm.avg <- mean(subset(qc.lot.tms, Name=='yeastRNA')$MedianTm)
tm.norm <- merge(data.frame(tm.spread[,1:4], tm.spread[,5:10]/tm.spread$yeastRNA), qc.lot.tms[qc.lot.tms$Name=='yeastRNA',c('PouchLotNumber','MedianTm')], by.x='LotNo', by.y='PouchLotNumber')
tm.norm <- data.frame(RunDataId = tm.spread$RunDataId, tm.spread[,5:10]*tm.norm$MedianTm/qc.yeast.tm.avg)
colnames(tm.norm)[2:7] <- paste(colnames(tm.norm[2:7]), 'Tm', sep='_')
max(tm.norm[,3:7])
head(tm.norm)
max(tm.norm[,2:7], na.rm=TRUE)
tm.sparse.handler <- 100 # TRY TUNING THIS????
tm.norm[,c(2:7)][is.na(tm.norm[,c(2:7)])] <- tm.sparse.handler
dat.norm <- merge(cp.norm, tm.norm, by='RunDataId')
head(dat.norm)
site.rhino.count <- with(merge(data.frame(cp.spread, Positive=1), calendar.df, by='Date'), aggregate(Positive~YearWeek+CustomerSiteId, FUN=sum))
sites <- as.character(unique(site.rhino.count$CustomerSiteId))[order(as.character(unique(site.rhino.count$CustomerSiteId)))]
site.rhino.count <- do.call(rbind, lapply(1:length(sites), function(x) data.frame(merge(data.frame(YearWeek = unique(calendar.df[,c('YearWeek')]), CustomerSiteId = sites[x]), site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], c('YearWeek','Positive')], by='YearWeek', all.x=TRUE))))
site.rhino.count[is.na(site.rhino.count$Positive), 'Gap'] <- 1
site.rhino.count[is.na(site.rhino.count$Gap), 'Gap'] <- 0
periods <- unique(as.character(site.rhino.count$YearWeek))
site.gaps <- do.call(rbind, lapply(1:length(sites), function(x) do.call(rbind, lapply(5:length(periods), function(y) data.frame(YearWeek = periods[y], CustomerSiteId = sites[x], MissingPeriods = sum(site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], 'Gap'][(y-4):y]))))))
site.starts <- do.call(rbind, lapply(1:length(sites), function(x) site.gaps[site.gaps$CustomerSiteId==sites[x],][max(which(site.gaps[site.gaps$CustomerSiteId==sites[x], 'MissingPeriods']==5)), c('CustomerSiteId','YearWeek')]))
# dat.features <- dat.norm[,2:16]
dat.features <- merge(dat.norm, calendar.df[,c('Date','YearWeek')], by='Date')
dat.trim <- do.call(rbind, lapply(1:length(site.starts$CustomerSiteId), function(x) filter(dat.features, (CustomerSiteId == site.starts[x,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[x,'YearWeek'])))))
head(dat.trim)
head(dat.trim)
max(dat.trim$RunDataId)
max(dat.trim$RunDataId)
class(dat.trim$RunDataId)
head(dat.trime)
head(dat.trim)
head(dat.features)
setwd('~/FilmArrayTrend/EnteroD68/')
# load the neccessary libraries
library(RODBC)
library(lubridate)
library(ggplot2)
library(mgcv)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
library(rgl)
library(AnomalyDetection)
# Set up variables needed later in the analysis
# ===========================================================================================
# create an Epi date calendar that will be used by all the data sets
startYear <- 2013
calendar.df <- createCalendarLikeMicrosoft(startYear, 'Week')
calendar.df <- transformToEpiWeeks(calendar.df)
calendar.df$YearWeek <- with(calendar.df, ifelse(Week < 10, paste(Year, Week, sep='-0'), paste(Year, Week, sep='-')))
calendar.df$Days <- 1
# set up some constants
imgDir <- 'Figures/'
dateBreaks <- unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])[order(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek']))][seq(1, length(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])), 8)]
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- readLines('../DataSources/CustomerSiteIdsWithNames.sql')
query <- paste(queryVector ,collapse="\n")
custnames.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/sitesRunningRP.txt',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
sites.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/rpRunsBySite.sql',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
runs.df <- sqlQuery(FADWcxn,query)
odbcClose(FADWcxn)
PMScxn <- odbcConnect('PMS_PROD')
queryVector <- readLines('../DataSources/SQL/EnteroD68/qcMedianCpRP.sql')
query <- paste(queryVector, collapse = '\n')
qc.lot.cps <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/qcMedianTmRP.sql')
query <- paste(queryVector, collapse = '\n')
qc.lot.tms <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/rhinoDataAtCHLA.sql')
query <- paste(queryVector, collapse = '\n')
chla.df <- sqlQuery(PMScxn, query)
queryVector <- readLines('../DataSources/SQL/EnteroD68/CMH_SequencedRuns.sql')
query <- paste(queryVector, collapse = '\n')
cmh.df <- sqlQuery(PMScxn, query)
odbcClose(PMScxn)
dat.df <- c()
choose.sites <- as.character(sites.df[,'CustomerSiteId'])
for(j in 1:length(choose.sites)) {
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- readLines('../DataSources/SQL/EnteroD68/rhinoDataBySite.sql')
query <- paste(gsub('SITE_INDEX', choose.sites[j], queryVector), collapse="\n")
dat.site.df <- sqlQuery(FADWcxn, query)
odbcClose(FADWcxn)
dat.df <- rbind(dat.df, dat.site.df)
}
rm(dat.site.df)
# Clean the data
# ============================================================================================
# with the data, determine the median value of Tm, Cp, and Max Fluor for each assay in the HRV/EV target
cp.median <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=dat.df)
# ###########################
# CHLA data are already in the Trend database, so don't add them back, but the data can be used to ID runs that were flagged for testing
# this would be the same for Tm
# BUT... CHILDREN'S MERCY RUNS DON'T APPEAR TO BE IN TREND, SO ADD THEM BACK AND THEY CAN BE FLAGGED AS POS/NEG FOR EV/D68
# ###########################
cp.median.chla <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=chla.df)
cp.median.cmh <- aggregate(Cp~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=cmh.df)
cp.median <- rbind(rbind(cp.median, cp.median.chla), cp.median.cmh)
cp.spread <- spread(data = cp.median, key = AssayName, value = Cp)
tm.median <- aggregate(Tm~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=dat.df)
tm.median.chla <- aggregate(Tm~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=chla.df)
tm.median.cmh <- aggregate(Tm~RunDataId+LotNo+CustomerSiteId+Date+AssayName, FUN=median, data=cmh.df)
tm.median <- rbind(rbind(tm.median, tm.median.chla), tm.median.cmh)
tm.spread <- spread(data = tm.median, key = AssayName, value = Tm)
# Scale assay median Cp and Tm data using the yeast control as well as QC data where applicable
# ============================================================================================
if(FALSE) {
# cp.norm <- merge(data.frame(cp.spread[,1:4], cp.spread[,5:10]/cp.spread$yeastRNA), qc.lot.cps[qc.lot.cps$Name=='yeastRNA',c('PouchLotNumber','MedianCp')], by.x='LotNo', by.y='PouchLotNumber')
# cp.norm <- data.frame(cp.norm[,1:4], cp.norm[,5:10]*cp.norm$MedianCp)
# colnames(cp.norm)[5:10] <- paste(colnames(cp.norm[5:10]), 'Cp', sep='_')
# cp.norm[,c(5:10)][is.na(cp.norm[,c(5:10)])] <- cp.sparse.handler
# ### WHAT IS A BETTER NORMALIZATION TECHNIQUE FOR Cp AND Tm PROBABLY ALSO????
# qc.yeast.cp.avg <- mean(subset(qc.lot.cps, Name=='yeastRNA')$MedianCp)
# a <- merge(cp.spread, subset(qc.lot.cps, Name=='yeastRNA'), by.x='LotNo', by.y='PouchLotNumber')
# a$TryOneHRV1 <- a$HRV1/a$yeastRNA*a$MedianCp/qc.yeast.cp.avg
# ggplot(a, aes(x=Date, y=HRV1/yeastRNA, color='RunNorm (Cp/CpYeast)')) + geom_point() + geom_point(data=a, aes(x=Date, y=1.5*TryOneHRV1, color='YeastNormRunNorm (1.5*Cp/CpYeast*CpYeastQC/CpYeastQCAvg)'), alpha=0.1) + geom_point(data=a, aes(x=Date, y=yeastRNA/35, color='YeastRaw (CpYeast/35)'), alpha=0.1) + scale_color_manual(values=c('black','blue','red'))
# ### YES I LIKE THIS NORMALIZATION BETTER
} # old normalization method
qc.yeast.cp.avg <- mean(subset(qc.lot.cps, Name=='yeastRNA')$MedianCp)
cp.norm <- merge(data.frame(cp.spread[,1:4], cp.spread[,5:10]/cp.spread$yeastRNA), qc.lot.cps[qc.lot.cps$Name=='yeastRNA',c('PouchLotNumber','MedianCp')], by.x='LotNo', by.y='PouchLotNumber')
cp.norm <- data.frame(cp.norm[,1:4], cp.norm[,5:10]*cp.norm$MedianCp/qc.yeast.cp.avg)
colnames(cp.norm)[5:10] <- paste(colnames(cp.norm[5:10]), 'Cp', sep='_')
### WHAT IS THE APPROPRIATE SPARSE HANDLER.... CAN I PLAY WITH THIS??? MAY ONLY WANT TO DO FOR A FEW SITES....
# which sites are the "right sites"
# maybe the children's hospitals??? 13, 25, 26, 33... with 7 as a control or something like that???
#############################
cp.sparse.handler <- 3 # TRY TUNING THIS????
cp.norm[,c(5:10)][is.na(cp.norm[,c(5:10)])] <- cp.sparse.handler
if(FALSE) {
# tm.norm <- data.frame(RunDataId = tm.spread$RunDataId, tm.spread[,5:10]/tm.spread$yeastRNA)
# colnames(tm.norm)[2:7] <- paste(colnames(tm.norm[2:7]), 'Tm', sep='_')
# tm.norm[,c(2:7)][is.na(tm.norm[,c(2:7)])] <- tm.sparse.handler
# dat.norm <- merge(cp.norm, tm.norm, by='RunDataId')
} # old normalization method
qc.yeast.tm.avg <- mean(subset(qc.lot.tms, Name=='yeastRNA')$MedianTm)
tm.norm <- merge(data.frame(tm.spread[,1:4], tm.spread[,5:10]/tm.spread$yeastRNA), qc.lot.tms[qc.lot.tms$Name=='yeastRNA',c('PouchLotNumber','MedianTm')], by.x='LotNo', by.y='PouchLotNumber')
tm.norm <- data.frame(RunDataId = tm.spread$RunDataId, tm.spread[,5:10]*tm.norm$MedianTm/qc.yeast.tm.avg)
colnames(tm.norm)[2:7] <- paste(colnames(tm.norm[2:7]), 'Tm', sep='_')
tm.sparse.handler <- 100 # TRY TUNING THIS????
tm.norm[,c(2:7)][is.na(tm.norm[,c(2:7)])] <- tm.sparse.handler
dat.norm <- merge(cp.norm, tm.norm, by='RunDataId')
# Since the proposed algorithm requires a certain amount of training data, figure out which sites are "eligible" for detection
# ============================================================================================
site.rhino.count <- with(merge(data.frame(cp.spread, Positive=1), calendar.df, by='Date'), aggregate(Positive~YearWeek+CustomerSiteId, FUN=sum))
sites <- as.character(unique(site.rhino.count$CustomerSiteId))[order(as.character(unique(site.rhino.count$CustomerSiteId)))]
site.rhino.count <- do.call(rbind, lapply(1:length(sites), function(x) data.frame(merge(data.frame(YearWeek = unique(calendar.df[,c('YearWeek')]), CustomerSiteId = sites[x]), site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], c('YearWeek','Positive')], by='YearWeek', all.x=TRUE))))
site.rhino.count[is.na(site.rhino.count$Positive), 'Gap'] <- 1
site.rhino.count[is.na(site.rhino.count$Gap), 'Gap'] <- 0
periods <- unique(as.character(site.rhino.count$YearWeek))
site.gaps <- do.call(rbind, lapply(1:length(sites), function(x) do.call(rbind, lapply(5:length(periods), function(y) data.frame(YearWeek = periods[y], CustomerSiteId = sites[x], MissingPeriods = sum(site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], 'Gap'][(y-4):y]))))))
site.starts <- do.call(rbind, lapply(1:length(sites), function(x) site.gaps[site.gaps$CustomerSiteId==sites[x],][max(which(site.gaps[site.gaps$CustomerSiteId==sites[x], 'MissingPeriods']==5)), c('CustomerSiteId','YearWeek')]))
# I think that here I may need to keep the pouch lot number so I can easily tie it to run observations from sequenced samples
dat.features <- merge(dat.norm, calendar.df[,c('Date','YearWeek')], by='Date')
dat.trim <- do.call(rbind, lapply(1:length(site.starts$CustomerSiteId), function(x) filter(dat.features, (CustomerSiteId == site.starts[x,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[x,'YearWeek'])))))
head(dat.trim)
max(dat.trim$RunDataId)
initial.window <- 100
test.horizon <- 10
# sites.all <- sites
# sites <- sites.all[c(2, 5, 6, 10, 18)]
# sites <- sites.all[!(sites.all %in% sites)]
scored.df <- c()
sites
# Set the environment
# ===========================================================================================
setwd('~/FilmArrayTrend/EnteroD68/')
# load the neccessary libraries
library(RODBC)
library(lubridate)
library(ggplot2)
library(mgcv)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
library(rgl)
library(AnomalyDetection)
scored.df <- read.csv('scoredOutput_20170412.csv', header = TRUE, row.names = FALSE)
heaD(scored.df)
head(scored.df)
scored.df <- read.csv('scoredOutput_20170412.csv', header = TRUE)
head(scored.df)
ggplot(scored.df, aes(x=TestStartDate, y=TestNoise/TrainNoise)) + geom_point() + facet_wrap(~CustomerSiteId)
ggplot(scored.df, aes(x=TestStartDate, y=TestNoise/TrainNoise)) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y')
ggplot(scored.df, aes(x=TestStartDate, y=TestNoise/TrainNoise)) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y') + theme(axis.text.x=element_text(angle=90))
head(scored.df)
read.csv('../DataSources/SQL/EnteroD68/ChilderensMercy26_SequencedEVD68.csv')
cmh.seq.runs <- read.csv('../DataSources/SQL/EnteroD68/ChilderensMercy26_SequencedEVD68.csv')
head(cmh.seq.runs)
head(cmh.seq.runs)
head(scored.df)
head(dat.df)
head(cmh.seq.runs)
head(cmh.df)
merge(unique(cmh.df[,c('RunDataId',)]))
merge(unique(cmh.df[,c('PouchSerialNumber','RunDataId','CustomerSiteId')]), cmh.seq.runs, by.x='PouchSerialNumber', by.y='SerialNumber')
a <- merge(unique(cmh.df[,c('PouchSerialNumber','RunDataId','CustomerSiteId')]), cmh.seq.runs, by.x='PouchSerialNumber', by.y='SerialNumber')
View(a)
cmh.seq.runs <- merge(unique(cmh.df[,c('PouchSerialNumber','RunDataId','CustomerSiteId')]), cmh.seq.runs, by.x='PouchSerialNumber', by.y='SerialNumber')
rm(a)
head(scored.df, cmh.seq.runs)
head(scored.df)
head(cmh.seq.runs)
a <- merge(scored.df, cmh.seq.runs[,c('RunDataId','CustomerSiteId','Alg1_Present')], by=c('RunDataId','CustomerSiteId'), all.x=TRUE)
head(a)
a[is.na(a$Alg1_Present), 'Alg1_Present'] <- 0
ggplot(a, aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present))) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y') + theme(axis.text.x=element_text(angle=90))
head(a)
ggplot(a, aes(x=TestStartDate)) + geom_bar() + facet_wrap(~CustomerSiteId)
b <- data.frame(Date = as.POSIXct(a$TestStartDate), CustomerSiteId = a$CustomerSiteId, Record = 1)
head(b)
AnomalyDetectionTs(b[b$CustomerSiteId==26,c('Date','Record')], max_anoms=0.01, direction='both', plot=TRUE)
AnomalyDetectionTs(b[b$CustomerSiteId==26, c('Date','Record')], max_anoms=0.01, direction='both', plot=TRUE)
head(b)
b[b$CustomerSiteId==26, c('Date','Record')]
AnomalyDetectionTs(with(b[b$CustomerSiteId==26, ], aggregate(Record~Date, FUN=sum)), max_anoms=0.01, direction='both', plot=TRUE)
AnomalyDetectionTs(with(b[b$CustomerSiteId==2, ], aggregate(Record~Date, FUN=sum)), max_anoms=0.01, direction='both', plot=TRUE)
AnomalyDetectionTs(with(b[b$CustomerSiteId==sites[1], ], aggregate(Record~Date, FUN=sum)), max_anoms=0.01, direction='both', plot=TRUE)
sites[1]
sites
AnomalyDetectionTs(with(b[b$CustomerSiteId==10, ], aggregate(Record~Date, FUN=sum)), max_anoms=0.01, direction='both', plot=TRUE)
AnomalyDetectionTs(with(b[b$CustomerSiteId==13, ], aggregate(Record~Date, FUN=sum)), max_anoms=0.01, direction='both', plot=TRUE)
unique(b$CustomerSiteId)
AnomalyDetectionTs(with(b[b$CustomerSiteId==33, ], aggregate(Record~Date, FUN=sum)), max_anoms=0.01, direction='both', plot=TRUE)
AnomalyDetectionTs(with(b[b$CustomerSiteId==31, ], aggregate(Record~Date, FUN=sum)), max_anoms=0.01, direction='both', plot=TRUE)
positive.freq <- data.frame(Date = as.POSIXct(scored.df$TestStartDate), CustomerSiteId = scored.df$CustomerSiteId, Record = 1)
positive.freq <- with(positive.freq, aggregate(Record~Date+CustomerSiteId, FUN=sum))
scored.sites <- unique(scored.df$CustomerSiteId)
scored.sites
head(positive.freq)
AnomalyDetectionTs(subset(positive.freq, CustomerSiteId==scored.sites[1])[,c('Date','Record')], direction='pos', max_anoms=0.01, plot=FALSE)
attributes(AnomalyDetectionTs(subset(positive.freq, CustomerSiteId==scored.sites[1])[,c('Date','Record')], direction='pos', max_anoms=0.01, plot=FALSE))
AnomalyDetectionTs(subset(positive.freq, CustomerSiteId==scored.sites[1])[,c('Date','Record')], direction='pos', max_anoms=0.01, plot=FALSE)$anoms
class(AnomalyDetectionTs(subset(positive.freq, CustomerSiteId==scored.sites[1])[,c('Date','Record')], direction='pos', max_anoms=0.01, plot=FALSE)$anoms)
AnomalyDetectionTs(subset(positive.freq, CustomerSiteId==scored.sites[1])[,c('Date','Record')], direction='pos', max_anoms=0.01, plot=FALSE)$anoms
AnomalyDetectionTs(subset(positive.freq, CustomerSiteId==scored.sites[1])[,c('Date','Record')], direction='pos', max_anoms=0.01, plot=FALSE)$anoms
positive.anoms <- c()
for(i in 1:length(scored.sites)) {
temp <- AnomalyDetectionTs(subset(positive.freq, CustomerSiteId==scored.sites[i])[,c('Date','Record')], direction='pos', max_anoms=0.01, plot=FALSE)$anoms
if(nrow(temp)==0) { next() }
temp$CustomerSiteId <- scored.sites[i]
positive.anoms <- rbind(positive.anoms, temp)
}
positive.anoms
rm(b)
head(a)
head(positive.anoms)
b <- merge(a, positive.anoms, by.x=c('TestStartDate','CustomerSiteId'), by.y=c('timestamp','CustomerSiteId'), all.x=TRUE)
head(b)
b[!(is.na(b$anoms)), ]
head(b)
b[!(is.na(b$anoms)), 'anoms'] <- 1
b[is.na(b$anoms), 'anoms'] <- 0
head(b)
ggplot(b, aes(x=TestStartDate, y=TestNoise/TrainNoise, shape=as.factor(anoms), color=as.factor(Alg1_Present))) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y')
ggplot(b, aes(x=TestStartDate, y=TestNoise/TrainNoise, size=as.factor(anoms), color=as.factor(Alg1_Present))) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y')
ggplot(subset(b, anoms==1), aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present))) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y')
ggplot(subset(b, anoms==0), aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present))) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y')
ggplot(subset(b, anoms==0), aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present))) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y') + geom_point(aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present)), data=subset(b, anoms==1), size=2)
ggplot(subset(b, anoms==0), aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present))) + geom_point(alpha=0.2) + facet_wrap(~CustomerSiteId, scale='free_y') + geom_point(aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present)), data=subset(b, anoms==1), size=2)
ggplot(subset(b, anoms==0), aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present))) + geom_point(alpha=0.2) + facet_wrap(~CustomerSiteId, scale='free_y') + geom_point(aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present)), data=subset(b, anoms==1), size=2) + scale_color_manual(vlaues=c('blue','black'))
ggplot(subset(b, anoms==0), aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present))) + geom_point(alpha=0.2) + facet_wrap(~CustomerSiteId, scale='free_y') + geom_point(aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present)), data=subset(b, anoms==1), size=2) + scale_color_manual(values=c('blue','black'))
positive.anoms
ggplot(subset(b, anoms==0), aes(x=TestStartDate, y=TestNoise/TrainNoise)) + geom_point(alpha=0.2, color='black') + facet_wrap(~CustomerSiteId, scale='free_y') + geom_point(aes(x=TestStartDate, y=TestNoise/TrainNoise), data=subset(b, anoms==1), size=2, color='blue')
# load the neccessary libraries
library(RODBC)
library(lubridate)
library(ggplot2)
library(mgcv)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
library(rgl)
library(AnomalyDetection)
head(b)
rm(a)
scored.seq.id <- merge(scored.df, cmh.seq.runs[,c('RunDataId','CustomerSiteId','Alg1_Present')], by=c('RunDataId','CustomerSiteId'), all.x=TRUE)
scored.seq.id[is.na(scored.seq.id$Alg1_Present), 'Alg1_Present'] <- 0
ggplot(scored.seq.id, aes(x=TestStartDate, y=TestNoise/TrainNoise, color=as.factor(Alg1_Present))) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y') + theme(axis.text.x=element_text(angle=90))
rm(b)
scored.freq.seq.id <- merge(scored.seq.id, positive.anoms, by.x=c('TestStartDate','CustomerSiteId'), by.y=c('timestamp','CustomerSiteId'), all.x=TRUE)
scored.freq.seq.id[!(is.na(scored.freq.seq.id$anoms)), 'anoms'] <- 1
scored.freq.seq.id[is.na(scored.freq.seq.id$anoms), 'anoms'] <- 0
unique(scored.freq.seq.id$anoms)
scored.freq.seq.id <- merge(scored.seq.id, positive.anoms, by.x=c('TestStartDate','CustomerSiteId'), by.y=c('timestamp','CustomerSiteId'), all.x=TRUE)
unique(scored.freq.seq.id$anoms)
scored.freq.seq.id[!(is.na(scored.freq.seq.id$anoms)), 'anoms'] <- 1
scored.freq.seq.id[is.na(scored.freq.seq.id$anoms), 'anoms'] <- 0
head(scored.freq.seq.id)
ggplot(scored.freq.seq.id, aes(x=TestStartDate, y=TestNoise/TrainNoise+anoms)) + geom_point() + facet_wrap(~CustomerSiteId)
ggplot(scored.freq.seq.id, aes(x=TestStartDate, y=TestNoise/TrainNoise+anoms)) + geom_point() + facet_wrap(~CustomerSiteId) + theme(axis.text.x=element_text(angle=90))
class(scored.freq.seq.id$TestStartDate)
ggplot(scored.freq.seq.id, aes(x=as.Date(TestStartDate), y=TestNoise/TrainNoise+anoms)) + geom_point() + facet_wrap(~CustomerSiteId) + theme(axis.text.x=element_text(angle=90))
