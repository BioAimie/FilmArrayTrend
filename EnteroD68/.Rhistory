site.start
site.train.periods
site.train.periods[j]
45-12
site.train.periods[j-train.weeks]
site.train.periods[j]
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
head(site.train)
min(site.train$YearWeek)
max(site.train$YearWeek)
45-33
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
site.train.periods
site.train.periods[40]
j <- 40
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
if(nrow(site.train) < 50) { break() }
# preprocess data using the PCA method
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
site.train.periods[j]
j <- 60
site.train.periods[j]
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
if(nrow(site.train) < 50) { break() }
# preprocess data using the PCA method
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
head(b)
unique(b$YearWeek)
site.test.pca
head(site.test.pca)
with(data.frame(site.test.pca, Count=1), aggregate(Count~Cluster, FUN=sum))
head(b)
with(data.frame(b, Count=1), aggregate(Count~Cluster, FUN=sum))
with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
head(d)
with(d, aggregate(Count~YearWeek, FUN=sum))
d <- merge(d, with(d, aggregate(Count~YearWeek, FUN=sum)), by='YearWeek')
head(d)
d$Portion <- d$Count.x/d$Count.y
head(d)
ggplot(d, aes(x=YearWeek, y=Portion, fill=Cluster)) + geom_bar(stat='identity')
d[d$YearWeek=='2014-25', ]
head(d)
unique(d$YearWeek)
data.frame(YearWeek = unique(d$YearWeek), Weight = seq(1, train.weeks, 1))
data.frame(YearWeek = unique(d$YearWeek), Weight = seq(1, (train.weeks+test.weeks), 1))
data.frame(YearWeek = unique(d$YearWeek), Weight = seq(1, (train.weeks+test.weeks), 1))
data.frame(YearWeek = unique(d$YearWeek), Weight = seq(1, (train.weeks+test.weeks), 1)/(train.weeks+test.weeks))
w <- data.frame(YearWeek = unique(d$YearWeek), Weight = seq(1, (train.weeks+test.weeks), 1)/(train.weeks+test.weeks))
data.frame(YearWeek = unique(d$YearWeek), Weight = seq(1, (train.weeks+test.weeks), 1)/(train.weeks+test.weeks))head(d)
head(d)
head(w)
Weight
w
merge(subset(d, Cluster==0), w, by='YearWeek')
f <- merge(subset(d, Cluster==0), w, by='YearWeek')
f
f
f$Portion*f$Weight
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Cluster),'Portion'] <- 0
f$Portion*f$Weight
f
f[is.na(f$Cluster),'Portion'] <- 0
f$Portion*f$Weight
f
f[is.na(f$Portion),'Portion'] <- 0
f$Portion*f$Weight
sum(f$Portion*f$Weight)[1:(length(f$Portion*f$Weight)-1)]
(f$Portion*f$Weight)[1:(length(f$Portion*f$Weight)-1)]
sum((f$Portion*f$Weight)[1:(length(f$Portion*f$Weight)-1)])
mean((f$Portion*f$Weight)[1:(length(f$Portion*f$Weight)-1)])
f
f
sum(f$Weight)
data.frame(YearWeek = unique(d$YearWeek), Weight = seq(1, (train.weeks+test.weeks), 1)/(train.weeks+test.weeks))
seq(1, (train.weeks+test.weeks), 1)/(train.weeks+test.weeks)
seq(1, (train.weeks+test.weeks), 1)
sum(seq(1, (train.weeks+test.weeks), 1))
data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, (train.weeks+test.weeks), 1)/sum(seq(1, (train.weeks+test.weeks), 1)))
w <- data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
w
merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
d
merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Portion),'Portion'] <- 0
f
f$Portion*f$Weight
(f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]
sum((f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))])
j
j <- j+1
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
if(nrow(site.train) < 50) { break() }
# preprocess data using the PCA method
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
d <- with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- merge(d, with(d, aggregate(Count~YearWeek, FUN=sum)), by='YearWeek')
w <- data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Portion),'Portion'] <- 0
f
head(b)
d
d$Portion <- d$Count.x/d$Count.y
w <- data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Portion),'Portion'] <- 0
(f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]
f
sum((f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))])
sum((f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]) < f$Portion[is.na(f$Weight)]
site.train.periods[j+1]
print('For site', sites[i], 'at time period', site.train.periods[j+1], 'there would be an anomaly.', sep=' ')
paste('For site', sites[i], 'at time period', site.train.periods[j+1], 'there would be an anomaly.', sep=' ')
for(j in (train.weeks+1):(length(site.train.periods)-test.weeks)) {
print(j)
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
if(nrow(site.train) < 50) { break() }
# preprocess data using the PCA method
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
d <- with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- merge(d, with(d, aggregate(Count~YearWeek, FUN=sum)), by='YearWeek')
d$Portion <- d$Count.x/d$Count.y
w <- data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Portion),'Portion'] <- 0
if(sum((f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]) < f$Portion[is.na(f$Weight)]) {
message <- paste('For site', sites[i], 'at time period', site.train.periods[j+1], 'there would be an anomaly.', sep=' ')
print(message)
}
}
j
for(j in (train.weeks+1):(length(site.train.periods)-test.weeks)) {
print(j)
}
for(j in (train.weeks+1):(length(site.train.periods)-test.weeks)) {
print(j)
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
if(nrow(site.train) < 50) { break() }
# preprocess data using the PCA method
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
d <- with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- merge(d, with(d, aggregate(Count~YearWeek, FUN=sum)), by='YearWeek')
d$Portion <- d$Count.x/d$Count.y
w <- data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Portion),'Portion'] <- 0
if(sum((f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]) < f$Portion[is.na(f$Weight)]) {
message <- paste('For site', sites[i], 'at time period', site.train.periods[j+1], 'there would be an anomaly.', sep=' ')
print(message)
}
}
print(j)
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
if(nrow(site.train) < 50) { break() }
# preprocess data using the PCA method
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
d <- with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- merge(d, with(d, aggregate(Count~YearWeek, FUN=sum)), by='YearWeek')
d$Portion <- d$Count.x/d$Count.y
w <- data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Portion),'Portion'] <- 0
if(sum((f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]) < f$Portion[is.na(f$Weight)]) {
message <- paste('For site', sites[i], 'at time period', site.train.periods[j+1], 'there would be an anomaly.', sep=' ')
print(message)
}
f
f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]
(f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]
d
f
train.weeks
j
train.weeks+1
print(j)
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
train.weeks+2
j <- 14
print(j)
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.train.periods[j-train.weeks]
site.train.pca
site.train.periods
site.train.periods[j-train.weeks]
site.start
site.train.periods[j]
j
j <- 13
j
print(j)
site.train <- site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.train.periods[j-train.weeks]
site.train.periods
site.train.periods[j]
38-26
unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])
site.train.periods[j-train.weeks]
print(j)
site.train <- site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek > (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
print(length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
print(j)
site.train <- site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
j <- j+1
print(j)
site.train <- site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
j <- 13
print(j)
site.train <- site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
j <- j+1
print(j)
site.train <- site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
j <- j+1
print(j)
site.train <- site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
print(length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
site.train.periods
j <- train.weeks+1
j
print(j)
print(length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])))
j
print(paste('At j =', j, 'the number of periods =', length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])), sep=' '))
j <- j+1
print(paste('At j =', j, 'the number of periods =', length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])), sep=' '))
for(j in (train.weeks+1):(length(site.train.periods)-test.weeks)) {
j <- j+1
print(paste('At j =', j, 'the number of periods =', length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])), sep=' '))
}
for(j in (train.weeks+1):(length(site.train.periods)-test.weeks)) {
j <- j+1
print(paste('At j =', j, 'the number of periods =', length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])), sep=' '))
}
for(j in (train.weeks+2):(length(site.train.periods)-test.weeks)) {
j <- j+1
print(paste('At j =', j, 'the number of periods =', length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])), sep=' '))
}
for(j in (train.weeks+2):(length(site.train.periods)-test.weeks)) {
# j <- j+1
# print(paste('At j =', j, 'the number of periods =', length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])), sep=' '))
site.train <- site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
if(nrow(site.train) < 50) { break() }
# preprocess data using the PCA method
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
d <- with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- merge(d, with(d, aggregate(Count~YearWeek, FUN=sum)), by='YearWeek')
d$Portion <- d$Count.x/d$Count.y
w <- data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Portion),'Portion'] <- 0
if(sum((f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]) < f$Portion[is.na(f$Weight)]) {
message <- paste('For site', sites[i], 'at time period', site.train.periods[j+1], 'there would be an anomaly.', sep=' ')
print(message)
}
}
warnings()
for(j in (train.weeks+2):(length(site.train.periods)-test.weeks)) {
# j <- j+1
# print(paste('At j =', j, 'the number of periods =', length(unique(site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], 'YearWeek'])), sep=' '))
print(site.train.periods[j])
site.train <- site.features[site.features$YearWeek >= (site.train.periods[j-train.weeks]) & site.features$YearWeek <= site.train.periods[j], ]
site.test <- site.features[site.features$YearWeek == site.train.periods[j+1], ]
if(nrow(site.train) < 50) { break() }
# preprocess data using the PCA method
set.seed(1234)
period.pca.trans <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(period.pca.trans, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(period.pca.trans, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on minimizing the ratio of points considered as noise
guess.eps <- 0.01
guess.mpt <- 10
noise.threshold <- 0.10
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
while(noise.threshold < noise.ratio) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
}
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,colnames(site.train.pca)[grep('^PC', colnames(site.train.pca))]],
site.test.pca[,colnames(site.test.pca)[grep('^PC', colnames(site.test.pca))]]))
b <- rbind(data.frame(cbind(site.train[,c('YearWeek','CustomerSiteId')], site.train.pca)), data.frame(cbind(site.test[,c('YearWeek','CustomerSiteId')], site.test.pca)))
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
d <- with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- merge(d, with(d, aggregate(Count~YearWeek, FUN=sum)), by='YearWeek')
d$Portion <- d$Count.x/d$Count.y
w <- data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
f <- merge(subset(d, Cluster==0), w, by='YearWeek', all=TRUE)
f[is.na(f$Cluster),'Cluster'] <- as.factor(0)
f[is.na(f$Portion),'Portion'] <- 0
if(sum((f$Portion*f$Weight)[!(is.na((f$Portion*f$Weight)))]) < f$Portion[is.na(f$Weight)]) {
message <- paste('For site', sites[i], 'at time period', site.train.periods[j+1], 'there would be an anomaly.', sep=' ')
print(message)
}
}
b
head(b)
max(b$YearWeek)
j
head(d)
ggplot(b, aes(x=YearWeek, fill=Cluster)) + geom_bar()
with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- with(data.frame(b, Count=1), aggregate(Count~YearWeek+Cluster, FUN=sum))
d <- merge(d, with(d, aggregate(Count~YearWeek, FUN=sum)), by='YearWeek')
d$Portion <- d$Count.x/d$Count.y
data.frame(YearWeek = unique(d$YearWeek)[1:train.weeks], Weight = seq(1, train.weeks, 1)/sum(seq(1, train.weeks, 1)))
