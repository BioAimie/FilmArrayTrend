site.rhino.count[is.na(site.rhino.count$Gap), 'Gap'] <- 0
periods <- unique(as.character(site.rhino.count$YearWeek))
do.call(rbind, lapply(1:length(sites), function(x) do.call(rbind, lapply(5:length(periods), function(y) data.frame(YearWeek = periods[y], CustomerSiteId = sites[x], MissingPeriods = sum(site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], 'Gap'][(y-4):y]))))))
site.rhino.count[is.na(site.rhino.count$Positive), 'Gap'] <- 1
site.rhino.count[is.na(site.rhino.count$Gap), 'Gap'] <- 0
do.call(rbind, lapply(1:length(sites), function(x) do.call(rbind, lapply(5:length(periods), function(y) data.frame(YearWeek = periods[y], CustomerSiteId = sites[x], MissingPeriods = sum(site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], 'Gap'][(y-4):y]))))))
site.gaps <- do.call(rbind, lapply(1:length(sites), function(x) do.call(rbind, lapply(5:length(periods), function(y) data.frame(YearWeek = periods[y], CustomerSiteId = sites[x], MissingPeriods = sum(site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], 'Gap'][(y-4):y]))))))
head(site.gaps)
do.call(rbind, lapply(1:length(sites), function(x) site.gaps[site.gaps$CustomerSiteId==sites[x],][max(which(site.gaps[site.gaps$CustomerSiteId==sites[x], 'MissingPeriods']==5)), c('CustomerSiteId','YearWeek')]))
head(site.gaps)
site.gaps[site.gaps$CustomerSiteId==10, ]
site.gaps[site.gaps$CustomerSiteId==25, ]
site.starts
# Set the environment
# ===========================================================================================
setwd('~/FilmArrayTrend/EnteroD68/')
# load the neccessary libraries
library(RODBC)
library(lubridate)
library(ggplot2)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
library(rgl)
library(AnomalyDetection)
# Set up variables needed later in the analysis
# ===========================================================================================
# create an Epi date calendar that will be used by all the data sets
startYear <- 2013
calendar.df <- createCalendarLikeMicrosoft(startYear, 'Week')
calendar.df <- transformToEpiWeeks(calendar.df)
calendar.df$YearWeek <- with(calendar.df, ifelse(Week < 10, paste(Year, Week, sep='-0'), paste(Year, Week, sep='-')))
# set up some constants
imgDir <- 'Figures/'
dateBreaks <- unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])[order(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek']))][seq(1, length(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])), 8)]
# Load in the data
# ===========================================================================================
# set some query variables, like the customer site... also, get the number of RP runs by site
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- scan('../DataSources/SQL/EnteroD68/sitesRunningRP.txt',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
sites.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/rpRunsBySite.sql',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
runs.df <- sqlQuery(FADWcxn,query)
odbcClose(FADWcxn)
# start a loop to gather Cp data for all sites running RP
cp.df <- c()
choose.sites <- as.character(sites.df[,'CustomerSiteId'])
for(j in 1:length(choose.sites)) {
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- scan('../DataSources/SQL/EnteroD68/rhinoDataBySite.sql', what=character(), quote="")
query <- paste(gsub('SITE_INDEX', choose.sites[j], queryVector), collapse=" ")
cp.site.df <- sqlQuery(FADWcxn, query)
odbcClose(FADWcxn)
cp.df <- rbind(cp.df, cp.site.df)
}
rm(cp.site.df)
# Clean the data
# ===========================================================================================
# with the cp data, determine the median Cp of each assay in the HRV/EV target
cp.median <- aggregate(Cp~RunDataId+CustomerSiteId+Date+AssayName, FUN=median, data=cp.df)
cp.spread <- spread(data = cp.median, key = AssayName, value = Cp)
sparse.handler <- 40
cp.spread[,c(4:9)][is.na(cp.spread[,c(4:9)])] <- sparse.handler
# determine the sequence associated with each HRV/EV positive
run.ids <- unique(cp.median$RunDataId)
cp.ordered <- do.call(rbind, lapply(1:length(run.ids), function(x) data.frame(cp.median[cp.median$RunDataId==run.ids[x], ][order(cp.median[cp.median$RunDataId==run.ids[x], 'Cp']), ], Index = seq(1, length(cp.median[cp.median$RunDataId==run.ids[x], 'Cp']), 1))))
cp.sequence <- do.call(rbind, lapply(1:length(run.ids), function(x) data.frame(RunDataId = run.ids[x], Sequence = paste(as.character(cp.ordered[cp.ordered$RunDataId==run.ids[x], 'AssayName']), collapse=', '))))
# since the final algorithm should be run on a site-by-site basis, determine which data by site are "eligible"
site.rhino.count <- with(merge(data.frame(cp.spread, Positive=1), calendar.df, by='Date'), aggregate(Positive~YearWeek+CustomerSiteId, FUN=sum))
sites <- as.character(unique(site.rhino.count$CustomerSiteId))[order(as.character(unique(site.rhino.count$CustomerSiteId)))]
site.rhino.count <- do.call(rbind, lapply(1:length(sites), function(x) data.frame(merge(data.frame(YearWeek = unique(calendar.df[,c('YearWeek')]), CustomerSiteId = sites[x]), site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], c('YearWeek','Positive')], by='YearWeek', all.x=TRUE))))
site.rhino.count[is.na(site.rhino.count$Positive), 'Gap'] <- 1
site.rhino.count[is.na(site.rhino.count$Gap), 'Gap'] <- 0
periods <- unique(as.character(site.rhino.count$YearWeek))
site.gaps <- do.call(rbind, lapply(1:length(sites), function(x) do.call(rbind, lapply(5:length(periods), function(y) data.frame(YearWeek = periods[y], CustomerSiteId = sites[x], MissingPeriods = sum(site.rhino.count[site.rhino.count$CustomerSiteId==sites[x], 'Gap'][(y-4):y]))))))
site.starts <- do.call(rbind, lapply(1:length(sites), function(x) site.gaps[site.gaps$CustomerSiteId==sites[x],][max(which(site.gaps[site.gaps$CustomerSiteId==sites[x], 'MissingPeriods']==5)), c('CustomerSiteId','YearWeek')]))
# check to see if any of the features have near-zero variance (i.e. variables with very few unique values, which can skew results when data are split for train/test)
nzv <- nearZeroVar(cp.spread[,c(4:9)], saveMetrics = TRUE)
remove.vars <- row.names(nzv[nzv$nzv==TRUE,])
cp.clean <- cp.spread[,!(colnames(cp.spread) %in% remove.vars)]
# check to see if any of the variables have very strong correlation (cut off of 0.8)
keep.vars <- cor(cp.clean[,c(4:7)])[,-findCorrelation(cor(cp.clean[,c(4:7)]), cutoff=0.8)]
cp.clean <- cp.clean[,colnames(cp.clean) %in% c('RunDataId','Date','YearWeek','CustomerSiteId',row.names(keep.vars))]
head(cp.sequence)
head(cp.df)
head(cp.spread)
a <- merge(cp.spread, cp.sequence, by='RunDataId')
a$Count <- 1
head(a)
a[grep('^HRV$|^HRV4, HRV1$|^HRV4, HRV1, HRV2$|^HRV4, HRV1, HRV2, HRV3$', a$Sequence), ]
ggplot(a[grep('^HRV$|^HRV4, HRV1$|^HRV4, HRV1, HRV2$|^HRV4, HRV1, HRV2, HRV3$', a$Sequence), ], aes(x=Date, y=Count)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId, scale='free_y')
head(a)
head(calendar.df)
ggplot(with(merge(a[grep('^HRV$|^HRV4, HRV1$|^HRV4, HRV1, HRV2$|^HRV4, HRV1, HRV2, HRV3$', a$Sequence), ], calendar.df, by='Date'), aggregate(Count~YearWeek, FUN=sum)), aes(x=YearWeek, y=Count)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId, scale='free_y')
ggplot(with(merge(a[grep('^HRV$|^HRV4, HRV1$|^HRV4, HRV1, HRV2$|^HRV4, HRV1, HRV2, HRV3$', a$Sequence), ], calendar.df, by='Date'), aggregate(Count~YearWeek+CustomerSiteId, FUN=sum)), aes(x=YearWeek, y=Count)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId, scale='free_y')
cp.features <- merge(cp.spread, calendar.df[,c('Date','YearWeek')], by='Date')
cp.features <- cp.features[with(cp.features, order(CustomerSiteId, Date)), ]
sites <- unique(cp.features$CustomerSiteId)[order(unique(cp.features$CustomerSiteId))]
sites[c(5,8,9,13)]
sites <- sites[c(5,8,9,13)]
initial.window <- 100
test.horizon <- 10
i <- 1
sites[i]
site.start <- as.character(site.starts[site.starts$CustomerSiteId==sites[i], 'YearWeek'])
site.features <- cp.features[cp.features$CustomerSiteId==sites[i] & as.character(cp.features$YearWeek) > site.start, ]
site.features <- site.features[with(site.features, order(Date)), ]
site.features$Obs <- seq(1, length(site.features$Date), 1)
head(cp.features)
site.start <- as.character(site.starts[site.starts$CustomerSiteId==sites[i], 'YearWeek'])
site.features <- cp.features[cp.features$CustomerSiteId==sites[i] & as.character(cp.features$YearWeek) > site.start, ]
site.features <- site.features[with(site.features, order(Date)), ]
site.features$Obs <- seq(1, length(site.features$Date), 1)
seq(1, length(site.features$Date), 1)
length(site.features$Date)
head(site.features)
site.start
site.starts
length(site.features)
nrow(site.features)
sites
sites <- unique(cp.features$CustomerSiteId)[order(unique(cp.features$CustomerSiteId))]
sites
i
site.start <- as.character(site.starts[site.starts$CustomerSiteId==sites[i], 'YearWeek'])
site.features <- cp.features[cp.features$CustomerSiteId==sites[i] & as.character(cp.features$YearWeek) > site.start, ]
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { break() }
site.features$Obs <- seq(1, length(site.features$Date), 1)
head(sites.df)
head(site.features)
j
initial.window+1
j<-initial.window+1
j
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
head(site.train.pca)
head(site.df)
site.df <- c()
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
guess.rew
guess.res
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca, site.test.pca))
site.train.pca
head(site.test.pca)
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca, site.test.pca))
head(site.train.pca)
site.train.pca[,(colnames(site.train.pca) %in% grep('^PC', colnames(site.train.pca)))]
grep('^PC', colnames(site.train.pca))
site.train.pca[,grep('^PC', colnames(site.train.pca))]
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,grep('^PC', colnames(site.train.pca))], site.test.pca))
head(site.test.pca)
site.test.pca[site.test.pca$Cluster==0, ]
grep('^PC', colnames(site.train.pca))]
grep('^PC', colnames(site.train.pca))
max(grep('^PC', colnames(site.train.pca)))
min.week <- min(site.train$YearWeek)
max.week <- max(site.test$YearWeek)
min.week
max.week
j
pca.count <- max(grep('^PC', colnames(site.train.pca)))
train.noise <- nrow(site.train.pca[site.train.pca$Cluster==0, ])
test.noise <- nrow(site.test.pca[site.test.pca$Cluster==0, ])
temp <- data.frame(Seq = j, TrainNoise = train.noise, TestNoise = test.noise, PCAs = pca.count)
temp
site.df <- c()
site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
print(j)
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,grep('^PC', colnames(site.train.pca))], site.test.pca))
# count the number of clusters in the test set that are considered noise
pca.count <- max(grep('^PC', colnames(site.train.pca)))
train.noise <- nrow(site.train.pca[site.train.pca$Cluster==0, ])
test.noise <- nrow(site.test.pca[site.test.pca$Cluster==0, ])
temp <- data.frame(Seq = j, TrainNoise = train.noise, TestNoise = test.noise, PCAs = pca.count)
site.df <- rbind(site.df, temp)
}
print(Sys.time() - site.start.time)
temp
j
head(site.features)
max(site.features$Obs)
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
head(site.train)
nearZeroVar(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], saveMetrics = TRUE)
train.nzv <- nearZeroVar(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], saveMetrics = TRUE)
train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
train.remove.vars
head(site.train)
site.train[,!(colnames(site.train) %in% train.remove.vars)]
site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
head(site.train)
cor(site.train[,c(4:7)])[,-findCorrelation(cor(site.train[,c(4:7)]), cutoff=0.8)]
print(j)
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
train.nzv <- nearZeroVar(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], saveMetrics = TRUE)
train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,grep('^PC', colnames(site.train.pca))], site.test.pca))
pca.count <- max(grep('^PC', colnames(site.train.pca)))
train.noise <- nrow(site.train.pca[site.train.pca$Cluster==0, ])
test.noise <- nrow(site.test.pca[site.test.pca$Cluster==0, ])
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TrainNoise = train.noise, TestNoise = test.noise, PCAs = pca.count)
site.df <- rbind(site.df, temp)
site.df <- c()
site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
print(j)
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
train.nzv <- nearZeroVar(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], saveMetrics = TRUE)
train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,grep('^PC', colnames(site.train.pca))], site.test.pca))
# count the number of clusters in the test set that are considered noise
pca.count <- max(grep('^PC', colnames(site.train.pca)))
train.noise <- nrow(site.train.pca[site.train.pca$Cluster==0, ])
test.noise <- nrow(site.test.pca[site.test.pca$Cluster==0, ])
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TrainNoise = train.noise, TestNoise = test.noise, PCAs = pca.count)
site.df <- rbind(site.df, temp)
}
print(Sys.time() - site.start.time)
head(site.df)
ggplot(site.df, aes(x=Seq, y=TestNoise)) + geom_bar(stat='identity')
sites[i]
head(site.features)
ggplot(site.df, aes(x=Seq, y=TrainNoise)) + geom_bar(stat='identity')
head(site.df)
ggplot(site.df, aes(x=Seq, y=TestNoise, fill=as.factor(PCAs))) + geom_bar(stat='identity')
head(a)
head(site.df)
sd(site.df$TestNoise)
mean(site.df$TestNoise)
site.df[site.df$TestNoise > (mean(site.df$TestNoise) + site.df$PCAs*sd(site.df$TestNoise)), ]
site.df[site.df$TestNoise > (mean(site.df$TestNoise) + site.df$PCAs*sd(site.df$TestNoise)), ]
head(site.features)
site.features[site.features$Obs %in% c()646, 647, 1585, 1586), ]
site.features[site.features$Obs %in% c(646, 647, 1585, 1586), ]
scored.df <- c()
for (i in 1:length(sites)) {
# parition the data by site and set up a timeframe
site.start <- as.character(site.starts[site.starts$CustomerSiteId==sites[i], 'YearWeek'])
site.features <- cp.features[cp.features$CustomerSiteId==sites[i] & as.character(cp.features$YearWeek) > site.start, ]
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { break() }
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
train.nzv <- nearZeroVar(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], saveMetrics = TRUE)
train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,grep('^PC', colnames(site.train.pca))], site.test.pca))
# count the number of clusters in the test set that are considered noise
pca.count <- max(grep('^PC', colnames(site.train.pca)))
train.noise <- nrow(site.train.pca[site.train.pca$Cluster==0, ])
test.noise <- nrow(site.test.pca[site.test.pca$Cluster==0, ])
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TrainNoise = train.noise, TestNoise = test.noise, PCAs = pca.count)
site.df <- rbind(site.df, temp)
}
site.df$PCTrainWeightedScore <- mean(site.df$TrainNoise) + site.df$PCAs*sd(site.df$TrainNoise)
site.df$PCTestWeightedScore <- mean(site.df$TestNoise) + site.df$PCAs*sd(site.df$TestNoise)
print(Sys.time() - site.start.time)
scored.df <- rbind(scored.df, site.df)
}
head(scored.df)
ggplot(scored.df, aes(x=Seq, y=PCATestWeightedScore)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId)
ggplot(scored.df, aes(x=Seq, y=PCTestWeightedScore)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId)
sites
i
j
sites[i]
head(site.df)
j
i
i <- 6
# parition the data by site and set up a timeframe
site.start <- as.character(site.starts[site.starts$CustomerSiteId==sites[i], 'YearWeek'])
site.features <- cp.features[cp.features$CustomerSiteId==sites[i] & as.character(cp.features$YearWeek) > site.start, ]
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { next }
site.features$Obs <- seq(1, length(site.features$Date), 1)
head(site.features)
site.features[site.features$Obs==j, ]
site.features[site.features$Obs==j, 'YearWeek']
scored.df <- c()
for (i in 1:length(sites)) {
# parition the data by site and set up a timeframe
site.start <- as.character(site.starts[site.starts$CustomerSiteId==sites[i], 'YearWeek'])
site.features <- cp.features[cp.features$CustomerSiteId==sites[i] & as.character(cp.features$YearWeek) > site.start, ]
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { next }
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
train.nzv <- nearZeroVar(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], saveMetrics = TRUE)
train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.pca, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.pca$Cluster <- as.factor(guess.res$cluster)
site.test.pca$Cluster <- unname(predict(guess.res, site.train.pca[,grep('^PC', colnames(site.train.pca))], site.test.pca))
# count the number of clusters in the test set that are considered noise
pca.count <- max(grep('^PC', colnames(site.train.pca)))
train.noise <- nrow(site.train.pca[site.train.pca$Cluster==0, ])
test.noise <- nrow(site.test.pca[site.test.pca$Cluster==0, ])
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TrainNoise = train.noise, TestNoise = test.noise, PCAs = pca.count)
site.df <- rbind(site.df, temp)
}
site.df$PCTrainWeightedScore <- mean(site.df$TrainNoise) + site.df$PCAs*sd(site.df$TrainNoise)
site.df$PCTestWeightedScore <- mean(site.df$TestNoise) + site.df$PCAs*sd(site.df$TestNoise)
print(Sys.time() - site.start.time)
scored.df <- rbind(scored.df, site.df)
}
head(scored.df)
ggplot(scored.df, aes(x=Seq, y=PCTestWeightedScore)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId)
ggplot(scored.df, aes(x=Seq, y=PCTestWeightedScore)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId, scale='free')
ggplot(scored.df, aes(x=Seq, y=PCTrainWeightedScore)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId, scale='free')
ggplot(scored.df, aes(x=Seq, y=(PCTrainWeightedScore-PCTestWeightedScore))) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId, scale='free')
ggplot(scored.df, aes(x=Seq, y=TestNoise)) + geom_bar(stat='identity') + facet_wrap(~CustomerSiteId, scale='free')
head(cp.features)
sites
sites[9]
i <- 9
site.start <- as.character(site.starts[site.starts$CustomerSiteId==sites[i], 'YearWeek'])
site.features <- cp.features[cp.features$CustomerSiteId==sites[i] & as.character(cp.features$YearWeek) > site.start, ]
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { next }
site.features$Obs <- seq(1, length(site.features$Date), 1)
head(site.features)
ggplot(subset(site.features, Obs >= 101), aes(x=YearWeek)) + geom_bar()
ggplot(subset(site.features, Obs >= 101), aes(x=Obs)) + geom_bar()
head(scored.df)
scored.df[scored.df$CustomerSiteId==26 & scored.df$TestNoise > 3, ]
ggplot(scored.df[scored.df$CustomerSiteId==26, ], aes(x=Seq, y=TestNoise)) + geom_bar(stat='identity')
head(site.features)
head(cp.sequence)
head(a)
a[a$CustomerSiteId==26, 'RunDataId']
head(site.features)
a <- merge(site.features, cp.sequence, by='RunDataId')
head(a)
a[grep('^HRV4$|^HRV4, HRV1$|^HRV4, HRV1, HRV3$|^HRV4, HRV1, HRV3, HRV4$', a$Sequence), ]
a[grep('^HRV4$|^HRV4, HRV1$|^HRV4, HRV1, HRV3$|^HRV4, HRV1, HRV3, HRV4$', a$Sequence), ]
a$Flag <- NA
a[grep('^HRV4$|^HRV4, HRV1$|^HRV4, HRV1, HRV3$|^HRV4, HRV1, HRV3, HRV4$', a$Sequence), 'Flag']
a[grep('^HRV4$|^HRV4, HRV1$|^HRV4, HRV1, HRV3$|^HRV4, HRV1, HRV3, HRV4$', a$Sequence), 'Flag'] <- 'Alg1'
a[is.na(a$Flag),'Flag'] <- 'NotAlg1'
head(a)
head(a)
head(a)
seq(1, length(a$RunDataId), 1)
a$Obs <- seq(1, length(a$RunDataId), 1)
head(a)
a[with(a, order(Date)), ]
a <- a[with(a, order(Date)), ]
a <- a[with(a, order(Date, RunDataId)), ]
head(a)
a$Obs <- seq(1, length(a$RunDataId), 1)
head(a)
ggplot(a, aes(x=Obs, fill=Flag)) + geom_bar()
View(a)
View(site.features)
head(site.features)
head(cp.sequence)
a <- merge(site.features, cp.sequence, by='RunDataId')
head(a)
a <- a[with(a, order(Date, RunDataId)), ]
head(a)
a[grep('^HRV4$|^HRV4, HRV1$|^HRV4, HRV1, HRV3$|^HRV4, HRV1, HRV3, HRV4$', a$Sequence), 'Flag'] <- 'Alg1'
a[is.na(a$Flag),'Flag'] <- 'NotAlg1'
head(a)
ggplot(a, aes(x=Obs, fill=Flag)) + geom_bar()
ggplot(a, aes(x=Obs, y=TestNoise, fill=Flag)) + geom_bar()
head(scored.df)
head(a)
head(site.features)
b <- merge(scored.df, a[,c('CustomerSiteId','Obs','Flag')], by.x=c('CustomerSiteId','Seq'), by.y=c('CustomerSiteId','Obs'))
head(b)
ggplot(b, aes(x=Seq, y=TestNoise, fill=Flag)) + geom_bar(stat='identity')
ggplot(b, aes(x=Seq, y=TestNoise, color=Flag)) + geom_point()
