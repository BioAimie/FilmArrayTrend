head(dat.features)
ggplot(dat.features, aes(x=Date, y=HRV4_Cp)) + geom_point() + facet_wrap(~CustomerSiteId)
head(dat.features)
sites
head(site.starts)
filter(dat.features, CustomerSiteId == sites.starts[1,'CustomerSiteId'])
filter(dat.features, CustomerSiteId == site.starts[1,'CustomerSiteId'])
filter(dat.features, CustomerSiteId == (site.starts[1,'CustomerSiteId'] & dat.features$Date > site.starts[1,'YearWeek']))
head(dat.features)
head(calendar.df)
dat.features <- merge(dat.features, calendar.df[,c('Date','YearWeek')], by='Date')
head(dat.features)
filter(dat.features, CustomerSiteId == (site.starts[1,'CustomerSiteId'] & dat.features$YearWeek > site.starts[1,'YearWeek']))
filter(dat.features, CustomerSiteId == (site.starts[1,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[1,'YearWeek'])))
class(dat.features$CustomerSiteId)
class(site.starts$CustomerSiteId)
filter(dat.features, (CustomerSiteId == site.starts[1,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[1,'YearWeek'])))
filter(dat.features, (CustomerSiteId == site.starts[1,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[1,'YearWeek'])))
site.starts[1,]
filter(dat.features, (CustomerSiteId == site.starts[2,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[2,'YearWeek'])))
do.call(rbind, lapply(1:length(site.starts$CustomerSiteId), function(x) filter(dat.features, (CustomerSiteId == site.starts[x,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[x,'YearWeek'])))))
dat.trim <- do.call(rbind, lapply(1:length(site.starts$CustomerSiteId), function(x) filter(dat.features, (CustomerSiteId == site.starts[x,'CustomerSiteId'] & as.character(dat.features$YearWeek) > as.character(site.starts[x,'YearWeek'])))))
head(dat.trim)
sites
sites[2]
scored.df <- c()
initial.window <- 100
test.horizon <- 10
i <- 2
sites[i]
site.features <- filter(dat.trim, CustomerSiteId == sites[i])
head(site.features)
site.features <- site.features[with(site.features, order(Date)), ]
nrow(site.features)
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
j <- initial.window+1
j
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
head(site.train)
head(site.test)
head(site.train)
head(cp.median)
unique(cp.median$AssayName)
as.character(unique(cp.median$AssayName))
paste(as.character(unique(cp.median$AssayName)), collapse='|')
grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))
colnames(site.train)[grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))]
colnames(site.train)[grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))]
grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))
head(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
train.remove.vars
nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
agg.nzv <- nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
agg.remove.vars <- row.names(agg.nzv[agg.nzv$nzv==TRUE,])
agg.remove.vars
train.remove.vars
head(site.train)
head(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','PCA'))
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','pca'))
preProcess(site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))], method=c('center','scale','pca'))
nearZeroVar(site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))], saveMetrics = TRUE)
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','pca'))$trace
attributes(preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','pca')))
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','pca'))$thresh
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','pca'))$pcaComp
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','pca'))$bagImp
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','pca'))$median
preProcess(site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))], method=c('center','scale','BoxCox'))
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
head(site.train)
hist(site.train$HRV1_Cp)
hist(site.train$HRV2_Cp)
hist(site.train$HRV3_Cp)
hist(site.train$HRV4_Cp)
hist(site.train$Entero1_Cp)
hist(site.train$Entero2_Cp)
preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
site.train.trans <- predict(bc.trans, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
site.test.trans <- predict(bc.trans, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
head(site.train.trans)
head(site.test.trans)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
guess.res
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
guess.res
noise.ratio
site.train.trans$Cluster <- as.factor(guess.res$cluster)
site.test.trans$Cluster <- unname(predict(guess.res, site.train.trans[,grep('^PC', colnames(site.train.trans))], site.test.trans))
as.factor(guess.res$cluster)
site.train.trans$Cluster <- as.factor(guess.res$cluster)
unname(predict(guess.res, site.train.trans, site.test.trans))
head(site.train.trans)
head(site.train.test)
head(site.test.trans)
unname(predict(guess.res, site.train.trans[,grep('Tm|Cp', colnames(site.train.trans))], site.test.trans))
site.test.trans$Cluster <- unname(predict(guess.res, site.train.trans[,grep('Tm|Cp', colnames(site.train.trans))], site.test.trans))
site.test.trans
nrow(site.train.trans[site.train.trans$Cluster==0, ])
nrow(site.test.trans[site.test.trans$Cluster==0, ])
head(site.train.trans)
head(site.train)
head(site.train)
unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))]
princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
summary(princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))]))
pca <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
attributes(pca)
plot(x=seq(1,12,1), y=pca$scores)
pca$scores
summary(pca)
pca$loadings
attributes(pca$loadings)
pca$loadings[[2]]
summary(pca)
atrributes(pca)
attributes(pca)
summary(pca)$sdev
summary(pca)$center
summary(pca)$scale
summary(pca)$scores
summary(pca)$call
summary(pca)
summary(pca)$loadings
summary(pca)$loadings[[1]]
summary(pca)$loadings[[12]]
pca
plot(x=seq(1,12,1), y=pca$sdev)
summary(pca)
pca$sdev[1]^2/sum(pca$sdev^2)
sapply(1:12, function(x) pca$sdev[x]^2/sum(pca$sdev^2))
plot(x=seq(1,12,1), y=sapply(1:12, function(x) pca$sdev[x]^2/sum(pca$sdev^2)))
rm(pca)
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))
which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05)
min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
pca.test <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
pca.test
pca.train
head(site.train)
min(site.train$Date)
max(site.test$Date) - min(site.train$Date)
as.numeric(max(site.test$Date) - min(site.train$Date))
110/as.numeric(max(site.test$Date) - min(site.train$Date))
site.df <- c()
site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
# split into train and test data
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
# THIS IS NEW... I'M JUST TESTING STUFF!!!!
# get the near zero variance of the train set and then the aggregate train + test set... the logic is that if the near zero variance
# changes when the data are aggregated, that *could* indicate a change, but this may just be too hair trigger tuned to EV
# train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
# train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
# agg.nzv <- nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
# agg.remove.vars <- row.names(agg.nzv[agg.nzv$nzv==TRUE,])
# nzv_score <- ifelse(length(agg.remove.vars) > length(train.remove.vars), 1, 0)
# site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
# site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
# pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
# site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
# site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# transform the data... center, scale, and use Box-Cox to transform the data
bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
site.train.trans <- predict(bc.trans, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
site.test.trans <- predict(bc.trans, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.trans$Cluster <- as.factor(guess.res$cluster)
site.test.trans$Cluster <- unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
# count the number of clusters in the test set that are considered noise
train.noise <- nrow(site.train.trans[site.train.trans$Cluster==0, ])
test.noise <- nrow(site.test.trans[site.test.trans$Cluster==0, ])
# figure out the number of Principle Components needed to explain at least 95% of the variance for the train set and then the
# train + test set...
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
# what other things would be good to measure differences? Perhaps frequency of tests? Like the test days??? How do we normalize for
# sites of different sizes??? How do we know if the frequency is abnormal??
#### FILL IN SOMETHING HERE IF I NEED TO...
# create some data frame that contains information about the timeslice
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TestStartDate = site.test[site.test$Obs==min(site.test$Obs), 'Date'],
RatioNoise = test.horizon*test.noise/train.noise, RatioPCA = pca.test/pca.train)
site.df <- rbind(site.df, temp)
}
sites[i]
sites.df
site.df
ggplot(site.df, aes(x=Date, y=RatioNoise)) + geom_point()
ggplot(site.df, aes(x=TestStartDate, y=RatioNoise)) + geom_point()
ggplot(site.df, aes(x=TestStartDate, y=RatioPCA)) + geom_point()
library(RODBC)
library(lubridate)
library(ggplot2)
library(mgcv)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
library(rgl)
library(AnomalyDetection)
head(temp)
head(site.df)
ggplot(site.df, aes=(x=Date, y=RatioNoise)) + geom_point()
ggplot(site.df, aes(x=Date, y=RatioNoise)) + geom_point()
ggplot(site.df, aes(x=TestStartDate, y=RatioNoise)) + geom_point()
site.df[site.df$RatioNoise>30, ]
site.df[site.df$RatioNoise>25, ]
ggplot(site.df, aes(x=TestStartDate, y=(RatioNoise+RatioPCA))) + geom_point()
site.df[(site.df$RatioNoise+site.df$RatioPCA) > 30, ]
sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))
a <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2))
sites
min(sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2)))
which(sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2)) <= 0.05)
sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2)[min(which(sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2)) <= 0.05))]
sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2))[min(which(sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2)) <= 0.05))]
1-sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2))[min(which(sapply(1:12, function(x) a$sdev[x]^2/sum(a$sdev^2)) <= 0.05))]
sites
sites[2]
i
sites
sites[c(2,6,9)]
sites[c(2,5,6,10,11)]
c(2, 5, 6, 10, 11)
for (i in c(2, 5, 6, 10, 11)) { #1:length(sites)) {
# parition the data by site and then order by the test date
site.features <- filter(dat.trim, CustomerSiteId == sites[i])
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { next }
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
# split into train and test data
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
# THIS IS NEW... I'M JUST TESTING STUFF!!!!
# get the near zero variance of the train set and then the aggregate train + test set... the logic is that if the near zero variance
# changes when the data are aggregated, that *could* indicate a change, but this may just be too hair trigger tuned to EV
# train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
# train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
# agg.nzv <- nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
# agg.remove.vars <- row.names(agg.nzv[agg.nzv$nzv==TRUE,])
# nzv_score <- ifelse(length(agg.remove.vars) > length(train.remove.vars), 1, 0)
# site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
# site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
# pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
# site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
# site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# transform the data... center, scale, and use Box-Cox to transform the data
bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
site.train.trans <- predict(bc.trans, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
site.test.trans <- predict(bc.trans, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.trans$Cluster <- as.factor(guess.res$cluster)
site.test.trans$Cluster <- unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
# count the number of clusters in the test set that are considered noise
train.noise <- nrow(site.train.trans[site.train.trans$Cluster==0, ])
test.noise <- nrow(site.test.trans[site.test.trans$Cluster==0, ])
# figure out the number of Principle Components needed to explain at least 95% of the variance for the train set and then the
# train + test set...
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train.var <- 1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
pca.train.count <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test.var <- 1-sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))[min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))]
pca.test.count <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
# what other things would be good to measure differences? Perhaps frequency of tests? Like the test days??? How do we normalize for
# sites of different sizes??? How do we know if the frequency is abnormal??
#### FILL IN SOMETHING HERE IF I NEED TO...
# create some data frame that contains information about the timeslice
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TestStartDate = site.test[site.test$Obs==min(site.test$Obs), 'Date'],
TrainNoise = train.noise, TestNoise = test.noise, RatioNoise = test.horizon*test.noise/train.noise,
TrainPCA = pca.train.count, TrainVar = pca.train.var, TestPCA = pca.test.count, TestVar = pca.test.var, RatioPCA = pca.test/pca.train)
site.df <- rbind(site.df, temp)
}
print(Sys.time() - site.start.time)
scored.df <- rbind(scored.df, site.df)
}
pca.test
site.train
head(site.train)
princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
pca.train.var <- 1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.train.count <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test.var <- 1-sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))[min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))]
pca.test.count <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
scored.df <- c()
# sites[c(2,5,6,10,11)]
for (i in c(2, 5, 6, 10, 11)) { #1:length(sites)) {
# parition the data by site and then order by the test date
site.features <- filter(dat.trim, CustomerSiteId == sites[i])
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { next }
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
# split into train and test data
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
# THIS IS NEW... I'M JUST TESTING STUFF!!!!
# get the near zero variance of the train set and then the aggregate train + test set... the logic is that if the near zero variance
# changes when the data are aggregated, that *could* indicate a change, but this may just be too hair trigger tuned to EV
# train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
# train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
# agg.nzv <- nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
# agg.remove.vars <- row.names(agg.nzv[agg.nzv$nzv==TRUE,])
# nzv_score <- ifelse(length(agg.remove.vars) > length(train.remove.vars), 1, 0)
# site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
# site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
# pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
# site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
# site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# transform the data... center, scale, and use Box-Cox to transform the data
bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
site.train.trans <- predict(bc.trans, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
site.test.trans <- predict(bc.trans, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.trans$Cluster <- as.factor(guess.res$cluster)
site.test.trans$Cluster <- unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
# count the number of clusters in the test set that are considered noise
train.noise <- nrow(site.train.trans[site.train.trans$Cluster==0, ])
test.noise <- nrow(site.test.trans[site.test.trans$Cluster==0, ])
# figure out the number of Principle Components needed to explain at least 95% of the variance for the train set and then the
# train + test set...
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train.var <- 1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
pca.train.count <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test.var <- 1-sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))[min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))]
pca.test.count <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
# what other things would be good to measure differences? Perhaps frequency of tests? Like the test days??? How do we normalize for
# sites of different sizes??? How do we know if the frequency is abnormal??
#### FILL IN SOMETHING HERE IF I NEED TO...
# create some data frame that contains information about the timeslice
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TestStartDate = site.test[site.test$Obs==min(site.test$Obs), 'Date'],
TrainNoise = train.noise, TestNoise = test.noise, RatioNoise = test.horizon*test.noise/train.noise,
TrainPCA = pca.train.count, TrainVar = pca.train.var, TestPCA = pca.test.count, TestVar = pca.test.var, RatioPCA = pca.test.count/pca.train.count)
site.df <- rbind(site.df, temp)
}
print(Sys.time() - site.start.time)
scored.df <- rbind(scored.df, site.df)
}
ggplot(site.df, aes(x=TestStartDate, y=(RatioNoise+RatioPCA))) + geom_point() + facet_wrap(~CustomerSiteId)
head(scored.df)
ggplot(scored.df, aes(x=TestStartDate, y=RatioNoise)) + geom_point() + facet_wrap(~CustomerSiteId)
ggplot(scored.df, aes(x=TestStartDate, y=(RatioNoise+RatioPCA/TestVar))) + geom_point() + facet_wrap(~CustomerSiteId)
head(cp.norm)
ggplot(subset(cp.norm, CustomerSiteId %in% sites[c(2,5,6,10,11)], aes(x=Date, y=Entero1_Cp)) + geom_point() + facet_wrap(~CustomerSiteId)
ggplot(subset(cp.norm, CustomerSiteId %in% sites[c(2,5,6,10,11)]), aes(x=Date, y=Entero1_Cp)) + geom_point() + facet_wrap(~CustomerSiteId)
ggplot(subset(cp.norm, CustomerSiteId %in% sites[c(2,5,6,10,11)]), aes(x=Date, y=Entero2_Cp)) + geom_point() + facet_wrap(~CustomerSiteId)
ggplot(subset(cp.norm, CustomerSiteId %in% sites[c(2,5,6,10,11)]), aes(x=Date, y=HRV4_Cp)) + geom_point() + facet_wrap(~CustomerSiteId)
head(scored.df)
hist(scored.df$TestVar/scored.df$TrainVar)
ggplot(scored.df, aes(x=TestStartDate, y=(RatioNoise*TestVar/TrainVar))) + geom_point() + facet_wrap(~CustomerSiteId)
sites[!(sites %in% sites[c(2,5,6,10,11)])]
which(sites!=sites[c(2,5,6,10,11)])
sites[which(sites!=sites[c(2,5,6,10,11)])]
scored.df <- c()
# sites[c(2,5,6,10,11)]
for (i in 1:length(sites)) { # c(2, 5, 6, 10, 11)) { #1:length(sites)) {
# parition the data by site and then order by the test date
site.features <- filter(dat.trim, CustomerSiteId == sites[i])
site.features <- site.features[with(site.features, order(Date)), ]
if(nrow(site.features)==0) { next }
site.features$Obs <- seq(1, length(site.features$Date), 1)
site.df <- c()
site.start.time <- Sys.time()
for(j in (initial.window+1):(length(site.features$Obs)-test.horizon)) {
# split into train and test data
site.train <- site.features[site.features$Obs < j & site.features$Obs >= (j - initial.window), ]
site.test <- site.features[site.features$Obs < (j + test.horizon) & site.features$Obs >= j, ]
# THIS IS NEW... I'M JUST TESTING STUFF!!!!
# get the near zero variance of the train set and then the aggregate train + test set... the logic is that if the near zero variance
# changes when the data are aggregated, that *could* indicate a change, but this may just be too hair trigger tuned to EV
# train.nzv <- nearZeroVar(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], saveMetrics = TRUE)
# train.remove.vars <- row.names(train.nzv[train.nzv$nzv==TRUE,])
# agg.nzv <- nearZeroVar(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]), saveMetrics = TRUE)
# agg.remove.vars <- row.names(agg.nzv[agg.nzv$nzv==TRUE,])
# nzv_score <- ifelse(length(agg.remove.vars) > length(train.remove.vars), 1, 0)
# site.train <- site.train[,!(colnames(site.train) %in% train.remove.vars)]
# site.test <- site.test[,!(colnames(site.test) %in% train.remove.vars)]
# pca.tranform <- preProcess(site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))], method = 'pca')
# site.train.pca <- predict(pca.tranform, site.train[,(colnames(site.train) %in% as.character(unique(cp.df$AssayName)))])
# site.test.pca <- predict(pca.tranform, site.test[,(colnames(site.test) %in% as.character(unique(cp.df$AssayName)))])
# transform the data... center, scale, and use Box-Cox to transform the data
bc.trans <- preProcess(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], method=c('center','scale','BoxCox'))
site.train.trans <- predict(bc.trans, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
site.test.trans <- predict(bc.trans, site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))])
# apply dbscan to the train data set... determine eps based on the point where there are 2 clusters (1 cluster + noise)
guess.eps <- 0.01
guess.mpt <- 90
eps.interval <- 0.01
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
cluster.int <- max(guess.res$cluster)
iter.start.time <- Sys.time()
while(cluster.int < 1) {
guess.eps <- guess.eps + eps.interval
guess.res <- dbscan(site.train.trans, eps = guess.eps, minPts = guess.mpt)
noise.ratio <- sum(guess.res$cluster==0)/length(guess.res$cluster)
cluster.int <- max(guess.res$cluster)
}
print(Sys.time() - iter.start.time)
# with the "correct" dbscan clustering, predict the clusters for the test data
site.train.trans$Cluster <- as.factor(guess.res$cluster)
site.test.trans$Cluster <- unname(predict(guess.res, site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test.trans))
# count the number of clusters in the test set that are considered noise
train.noise <- nrow(site.train.trans[site.train.trans$Cluster==0, ])
test.noise <- nrow(site.test.trans[site.test.trans$Cluster==0, ])
# figure out the number of Principle Components needed to explain at least 95% of the variance for the train set and then the
# train + test set...
pca.train <- princomp(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))])
pca.train.var <- 1-sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2))[min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))]
pca.train.count <- min(which(sapply(1:12, function(x) pca.train$sdev[x]^2/sum(pca.train$sdev^2)) <= 0.05))
pca.test <- princomp(rbind(site.train[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.train))], site.test[, grep(paste(as.character(unique(cp.median$AssayName)), collapse='|'), colnames(site.test))]))
pca.test.var <- 1-sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2))[min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))]
pca.test.count <- min(which(sapply(1:12, function(x) pca.test$sdev[x]^2/sum(pca.test$sdev^2)) <= 0.05))
# what other things would be good to measure differences? Perhaps frequency of tests? Like the test days??? How do we normalize for
# sites of different sizes??? How do we know if the frequency is abnormal??
#### FILL IN SOMETHING HERE IF I NEED TO...
# create some data frame that contains information about the timeslice
temp <- data.frame(CustomerSiteId = sites[i], Seq = j, TestStartDate = site.test[site.test$Obs==min(site.test$Obs), 'Date'],
TrainNoise = train.noise, TestNoise = test.noise, RatioNoise = test.horizon*test.noise/train.noise,
TrainPCA = pca.train.count, TrainVar = pca.train.var, TestPCA = pca.test.count, TestVar = pca.test.var, RatioPCA = pca.test.count/pca.train.count)
site.df <- rbind(site.df, temp)
}
print(Sys.time() - site.start.time)
scored.df <- rbind(scored.df, site.df)
}
head(scored.df)
ggplot(scored.df, aes(x=TestStartDate, y=(RatioNoise*TestVar/TrainVar))) + geom_point() + facet_wrap(~CustomerSiteId)
ggplot(scored.df, aes(x=TestStartDate, y=(RatioNoise*TestVar/TrainVar))) + geom_point() + facet_wrap(~CustomerSiteId, scale='free_y')
