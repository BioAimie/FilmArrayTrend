cp.df.HRV <- cp.rhino
HRV.spread <- aggregate(Cp~RunDataId+AssayName, data = cp.df.HRV, FUN = median)
head(HRV.spread)
HRV.spread <- spread(HRV.spread, AssayName, Cp)
install.packages(c('tidyr','dplyr'), dependencies = TRUE)
library(tidyr)
library(dplyr)
HRV.spread <- spread(HRV.spread, AssayName, Cp)
head(HRV.spread)
HRV.spread[,c(2:7)][is.na(HRV.spread[,c(2:7)])] <- 32
modelData <- HRV.spread
mydata <- modelData[,-1]
head(modelData)
head(mydata)
mydata <- modelData[,-1]
apply(mydata,2,var)
sum(apply(mydata,2,var))
(nrow(mydata)-1)
(nrow(mydata)-1)*sum(apply(mydata,2,var))
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata,
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
set.seed(8)
kModel <- kmeans(modelData[,-1],12 ,nstart = 10, iter.max = 100)
clusterData <- cbind(modelData, kModel$cluster)
names(clusterData)[length(clusterData)] <- "Cluster"
comboData <- inner_join(cp.df.HRV[,c("RunDataId","Date")], clusterData, by = "RunDataId")
names(comboData)[length(names(comboData))] <- "Cluster"
comboData <- comboData[!duplicated(interaction(comboData$RunDataId, comboData$Cluster)),]
head(cp.df.HRV)
head(clusterData)
comboData$Month <-  as.POSIXlt(as.Date(comboData$Date, "%Y-%m-%d"))
comboData$Month <- strftime(comboData$Month, format = "%Y-%W")
spreadTable <-  as.data.frame(table(comboData$Month, comboData$Cluster))
spreadTable
head(spreadTable)
head(comboData)
names(spreadTable) <- c("Date", "Cluster", "Freq")
spreadTable <- spread(spreadTable,Cluster, Freq)
head(spreadTable)
for(i in 1:nrow(spreadTable)){
sumRow = sum(spreadTable[i,c(2:length(spreadTable))])
spreadTable[i,c(2:length(spreadTable))] <- spreadTable[i,c(2:length(spreadTable))] / sumRow
}
gathered <- gather(spreadTable, Cluster, Freq, 2:length(spreadTable))
gathered$Date <- unlist(lapply(gathered$Date, as.character))
ggplot(gathered,aes(x = Date, y = Freq, colour = Cluster , group = Cluster)) + geom_line() +
theme(axis.text.x = element_text(angle = 90)) +
labs(title = "Rate of Clusters", x = "Date (Year-Week Number)", y = "Rate")
g <- ggplot(gathered,aes(x = Date, y = Freq, colour = Cluster , group = Cluster)) + geom_line() +
theme(axis.text.x = element_text(angle = 90)) +
labs(title = "Rate of Clusters", x = "Date (Year-Week Number)", y = "Rate")
plotly::plotly_build(g)
head(rhino.ml.features)
ggplot(rhino.features, aes(x=Date, y=Run, fill=as.factor(CustomerSiteId))) + geom_bar(stat='identity')
dev.off()
ggplot(rhino.features, aes(x=Date, y=Run, fill=as.factor(CustomerSiteId))) + geom_bar(stat='identity')
setwd('~/FilmArrayTrend/EnteroD68/')
# load the neccessary libraries
library(RODBC)
library(lubridate)
library(ggplot2)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
# create an Epi date calendar that will be used by all the data sets
startYear <- 2013
calendar.df <- createCalendarLikeMicrosoft(startYear, 'Week')
calendar.df <- transformToEpiWeeks(calendar.df)
calendar.df$YearWeek <- with(calendar.df, ifelse(Week < 10, paste(Year, Week, sep='-0'), paste(Year, Week, sep='-')))
# set up some constants
imgDir <- 'Figures/'
dateBreaks <- unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])[order(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek']))][seq(1, length(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])), 8)]
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- scan('../DataSources/SQL/EnteroD68/sitesRunningRP.txt',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
sites.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/rpRunsBySite.sql',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
runs.df <- sqlQuery(FADWcxn,query)
odbcClose(FADWcxn)
cp.df <- c()
choose.sites <- as.character(sites.df[,'CustomerSiteId'])
for(j in 1:length(choose.sites)) {
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- scan('../DataSources/SQL/EnteroD68/rhinoDataBySite.sql', what=character(), quote="")
query <- paste(gsub('SITE_INDEX', choose.sites[j], queryVector), collapse=" ")
cp.site.df <- sqlQuery(FADWcxn, query)
odbcClose(FADWcxn)
cp.df <- rbind(cp.df, cp.site.df)
}
setwd('~/FilmArrayTrend/EnteroD68/')
# load the neccessary libraries
library(RODBC)
library(lubridate)
library(ggplot2)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
# create an Epi date calendar that will be used by all the data sets
startYear <- 2013
calendar.df <- createCalendarLikeMicrosoft(startYear, 'Week')
calendar.df <- transformToEpiWeeks(calendar.df)
calendar.df$YearWeek <- with(calendar.df, ifelse(Week < 10, paste(Year, Week, sep='-0'), paste(Year, Week, sep='-')))
# set up some constants
imgDir <- 'Figures/'
dateBreaks <- unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])[order(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek']))][seq(1, length(unique(calendar.df[calendar.df$Year >= startYear, 'YearWeek'])), 8)]
# set some query variables, like the customer site... also, get the number of RP runs by site
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- scan('../DataSources/SQL/EnteroD68/sitesRunningRP.txt',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
sites.df <- sqlQuery(FADWcxn,query)
queryVector <- scan('../DataSources/SQL/EnteroD68/rpRunsBySite.sql',what=character(),quote="")
query <- paste(queryVector,collapse=" ")
runs.df <- sqlQuery(FADWcxn,query)
odbcClose(FADWcxn)
# start a loop to gather Cp data for all sites running RP
cp.df <- c()
choose.sites <- as.character(sites.df[,'CustomerSiteId'])
for(j in 1:length(choose.sites)) {
FADWcxn <- odbcConnect('FA_DW', uid = 'afaucett', pwd = 'ThisIsAPassword-BAD')
queryVector <- scan('../DataSources/SQL/EnteroD68/rhinoDataBySite.sql', what=character(), quote="")
query <- paste(gsub('SITE_INDEX', choose.sites[j], queryVector), collapse=" ")
cp.site.df <- sqlQuery(FADWcxn, query)
odbcClose(FADWcxn)
cp.df <- rbind(cp.df, cp.site.df)
}
rm(cp.site.df)
head(cp.df)
cp.median <- aggregate(Cp~RunDataId+CustomerSiteId+Date+AssayName, FUN=median, data=cp.df)
head(cp.median)
set.seed(10)
messy <- data.frame(
id = 1:4,
trt = sample(rep(c('control', 'treatment'), each = 2)),
work.T1 = runif(4),
home.T1 = runif(4),
work.T2 = runif(4),
home.T2 = runif(4)
)
head(messy)
head(cp.median)
head(cp.median[with(cp.median, order(RunDataId)), ])
write.csv(cp.median, file='medianCp_Raw.csv', row.names=FALSE)
messy
gather(key, time, -id, -trt)
messy %>% gather(key, time, -id, -trt)
messy %>% gather(key, time, -id, -trt)
head(cp.median)
spread(data = cp.median, key = AssayName, value = Cp, -RunDataId, -CustomerSiteId, -Date)
spread(data = cp.median, key = AssayName, value = Cp)
a <- spread(data = cp.median, key = AssayName, value = Cp)
head(cp.median)
head(a)
a[a$RunDataId==5, ]
filter(cp.median, RunDataId==5)
cp.spread <- spread(data = cp.median, key = AssayName, value = Cp)
rm(a)
head(cp.spread)
cp.spread[is.na(cp.spread[,c(3:8)]), ]
head(cp.spread)
head(cp.spread[,c(3:8)])
cp.spread[,c(3:8)][is.na(cp.spread[,c(3:8)])]
sparse.handler <- 40
cp.spread[,c(3:8)][is.na(cp.spread[,c(3:8)])] <- sparse.handler
head(cp.spread)
head(calendar.df)
a <- merge(cp.spread, calendar.df[,c('Date','YearWeek')], by='Date')
rm(a)
a <- merge(cp.spread, calendar.df[calendar.df$YearWeek >= '2013-26',c('Date','YearWeek')], by='Date')
head(a)
min(a$Date)
cp.clean <- merge(cp.spread, calendar.df[calendar.df$YearWeek >= '2013-26',c('Date','YearWeek')], by='Date')
rm(a)
head(cp.clean)
filter(calendar.df[,c('Date','YearWeek')], YearWeek >= '2013-26')
head(filter(calendar.df[,c('Date','YearWeek')], YearWeek >= '2013-26'))
cp.clean <- merge(cp.spread, filter(calendar.df[,c('Date','YearWeek')], YearWeek >= '2013-26'), by='Date')
head(cp.clean)
cp.spread <- spread(data = cp.median, key = AssayName, value = Cp)
sparse.handler <- 40
head(cp.spread)
cp.spread[,c(4:9)][is.na(cp.spread[,c(4:9)])] <- sparse.handler
cp.clean <- merge(cp.spread, filter(calendar.df[,c('Date','YearWeek')], YearWeek >= '2013-26'), by='Date')
head(cp.spread)
head(cp.clean)
hist(cp.clean$HRV1)
hist(cp.clean$HRV2)
hist(cp.clean$HRV3)
hist(cp.clean$HRV4)
hist(cp.clean$Entero1)
hist(cp.clean$Entero2)
nearZeroVar(cp.clean[,c(4:9)])
nzv <- nearZeroVar(cp.clean[,c(4:9)], saveMetrics = TRUE)
head(nzv)
filter(nzv, nzv = TRUE)
filter(nzv, nzv == TRUE)
names(filter(nzv, nzv == TRUE))
row.names(nzv)
row.names(nzv[nzv$nzv==TRUE,])
remove.vars <- row.names(nzv[nzv$nzv==TRUE,])
cp.clean[,!(colnames(cp.clean) %in% remove.vars)]
head(cp.clean[,!(colnames(cp.clean) %in% remove.vars)])
cp.clean <- cp.clean[,!(colnames(cp.clean) %in% remove.vars)]
head(cp.clean)
cor(cp.clean[,c(4:7)])
findCorrelation(cor(cp.clean[,c(4:7)]), cutoff=0.75)
findCorrelation(cor(cp.clean[,c(4:7)]), cutoff=0.8)
cor(cp.clean[,c(4:7)])[,-findCorrelation(cor(cp.clean[,c(4:7)]), cutoff=0.8)]
cor(cp.clean[,c(4:7)])[,-findCorrelation(cor(cp.clean[,c(4:7)]), cutoff=0.8)]
keep.vars <- cor(cp.clean[,c(4:7)])[,-findCorrelation(cor(cp.clean[,c(4:7)]), cutoff=0.8)]
cp.clean <- cp.clean[,colnames(cp.clean) %in% c('RunDataId','Date','YearWeek','CusomterSiteId',keep.vars)]
head(cp.clean)
keep.vars
cp.clean <- merge(cp.spread, filter(calendar.df[,c('Date','YearWeek')], YearWeek >= '2013-26'), by='Date')
nzv <- nearZeroVar(cp.clean[,c(4:9)], saveMetrics = TRUE)
remove.vars <- row.names(nzv[nzv$nzv==TRUE,])
cp.clean <- cp.clean[,!(colnames(cp.clean) %in% remove.vars)]
keep.vars
as.character(keep.vars)
keep.vars[1:4, ]
as.character(keep.vars[1:4, ])
as.character(keep.vars[, 1:4])
row.names(keep.vars)
keep.vars <- cor(cp.clean[,c(4:7)])[,-findCorrelation(cor(cp.clean[,c(4:7)]), cutoff=0.8)]
cp.clean <- cp.clean[,colnames(cp.clean) %in% c('RunDataId','Date','YearWeek','CusomterSiteId',row.names(keep.vars))]
head(cp.clean)
rhino.features <- cp.clean[,row.names(keep.vars)]
head(rhino.features)
feature.base <- cp.clean[,c('RunDataId','Date','YearWeek','CusomterSiteId')]
head(cp.clean)
cp.clean <- merge(cp.spread, filter(calendar.df[,c('Date','YearWeek')], YearWeek >= '2013-26'), by='Date')
# create some clusters using kNN from the caret package, utilizing 60% of the data for training and the rest for testing
#   first, check to see if any of the features have near-zero variance (i.e. variables with very few unique values, which can skew results when data are split for train/test)
nzv <- nearZeroVar(cp.clean[,c(4:9)], saveMetrics = TRUE)
remove.vars <- row.names(nzv[nzv$nzv==TRUE,])
cp.clean <- cp.clean[,!(colnames(cp.clean) %in% remove.vars)]
#   second, check to see if any of the variables have very strong correlation (cut off of 0.8)
keep.vars <- cor(cp.clean[,c(4:7)])[,-findCorrelation(cor(cp.clean[,c(4:7)]), cutoff=0.8)]
cp.clean <- cp.clean[,colnames(cp.clean) %in% c('RunDataId','Date','YearWeek','CustomerSiteId',row.names(keep.vars))]
feature.base <- cp.clean[,c('RunDataId','Date','YearWeek','CusomterSiteId')]
rhino.features <- cp.clean[,row.names(keep.vars)]
feature.base <- cp.clean[,c('RunDataId','Date','YearWeek','CustomerSiteId')]
head(cp.clean)
head(feature.base)
head(rhino.features)
trainIndex <- createDataPartition(cp.clean$RunDataId, p=0.6, list=FALSE, times=1)
cp.train <- cp.clean[trainIndex, ]
cp.test <- cp.clean[-trainIndex, ]
head(cp.clean)
head(cp.train)
feature.base[trainIndex, ]
features.train <- rhino.features[trainIndex, ]
head(cp.train)
head(base.t)
head(base.train)
base.train <- feature.base[trainIndex, ]
features.train <- rhino.features[trainIndex, ]
head(base.train)
head(features.train)
rm(feature.base)
rm(rhino.features)
set.seed(3456)
trainIndex <- createDataPartition(cp.clean$RunDataId, p=0.6, list=FALSE, times=1)
cp.train <- cp.clean[trainIndex, ]
cp.test <- cp.clean[-trainIndex, ]
base.train <- cp.clean[trainIndex, c('RunDataId','Date','YearWeek','CustomerSiteId')]
features.train <- cp.clean[-trainIndex, row.names(keep.vars)]
a <- preProcess(features.train, method = c('center','scale'))
head(a)
rm(a)
mdrr
preProcValues <- preProcess(features.train, method = 'pca')
features.train.trans <- predict(preProcValues, features.train)
set.seed(3456)
trainIndex <- createDataPartition(cp.clean$RunDataId, p=0.6, list=FALSE, times=1)
cp.train <- cp.clean[trainIndex, ]
cp.test <- cp.clean[-trainIndex, ]
base.train <- cp.clean[trainIndex, c('RunDataId','Date','YearWeek','CustomerSiteId')]
features.train <- cp.clean[trainIndex, row.names(keep.vars)]
base.test <- cp.clean[-trainIndex, c('RunDataId','Date','YearWeek','CustomerSiteId')]
features.test <- cp.clean[-trainIndex, row.names(keep.vars)]
preProcValues <- preProcess(features.train, method = 'pca')
features.train.trans <- predict(preProcValues, features.train)
features.test.trans <- predict(preProcValues, features.test)
head(features.train.trans)
head(features.test.trans)
View(cp.spread)
run.ids <- unique(cp.median$RunDataId)
cp.ordered <- do.call(rbind, lapply(1:length(run.ids), function(x) data.frame(cp.median[cp.median$RunDataId==run.ids[x], ][order(cp.median[cp.median$RunDataId==run.ids[x], 'MedianCp']), ], Index = seq(1, length(cp.median[cp.median$RunDataId==run.ids[x], 'MedianCp']), 1))))
cp.sequence <- do.call(rbind, lapply(1:length(run.ids), function(x) data.frame(RunDataId = run.ids[x], Sequence = paste(as.character(cp.ordered[cp.ordered$RunDataId==run.ids[x], 'AssayName']), collapse=', '))))
head(cp.median)
cp.ordered <- do.call(rbind, lapply(1:length(run.ids), function(x) data.frame(cp.median[cp.median$RunDataId==run.ids[x], ][order(cp.median[cp.median$RunDataId==run.ids[x], 'Cp']), ], Index = seq(1, length(cp.median[cp.median$RunDataId==run.ids[x], 'Cp']), 1))))
cp.sequence <- do.call(rbind, lapply(1:length(run.ids), function(x) data.frame(RunDataId = run.ids[x], Sequence = paste(as.character(cp.ordered[cp.ordered$RunDataId==run.ids[x], 'AssayName']), collapse=', '))))
head(cp.sequence)
cp.sequence[grep('^HRV4$', cp.sequence$Sequence), ]
cp.sequence[grep('^HRV4$|^HRV4,HRV1,HRV2,HRV3$', cp.sequence$Sequence), ]
head(cp.sequence)
cp.sequence[grep('^HRV4$|^HRV4, HRV1, HRV2, HRV3$', cp.sequence$Sequence), ]
cp.sequence[grep('^HRV4$|^HRV4, HRV1, HRV2, HRV3$', cp.sequence$Sequence), 'RunDataId']
cp.sequence[grep('^HRV4$', cp.sequence$Sequence), 'RunDataId']
cp.sequence[grep('^HRV4$|^HRV4, HRV1, HRV2, HRV3$|^HRv4, HRV1, HRV2$|^HRV4, HRV1$', cp.sequence$Sequence), 'RunDataId']
cp.sequence[grep('^HRV4$|^HRV4, HRV1, HRV2, HRV3$|^HRv4, HRV1, HRV2$|^HRV4, HRV1$', cp.sequence$Sequence), 'RunDataId']
clustered.df <- cbind(base, features.pca.classified)
set.seed(3456)
if(FALSE) {
# trainIndex <- createDataPartition(cp.clean$RunDataId, p=0.6, list=FALSE, times=1)
# cp.train <- cp.clean[trainIndex, ]
# cp.test <- cp.clean[-trainIndex, ]
# base.train <- cp.clean[trainIndex, c('RunDataId','Date','YearWeek','CustomerSiteId')]
# features.train <- cp.clean[trainIndex, row.names(keep.vars)]
# base.test <- cp.clean[-trainIndex, c('RunDataId','Date','YearWeek','CustomerSiteId')]
# features.test <- cp.clean[-trainIndex, row.names(keep.vars)]
} # if the data do need to be split into train and test....
base <- cp.clean[, c('RunDataId','Date','YearWeek','CustomerSiteId')]
features <- cp.clean[, row.names(keep.vars)]
#   preprocess the data using PCA option in caret... use the transformation on the training and test sets
preProcValues <- preProcess(features, method = 'pca')
features.pca <- predict(preProcValues, features)
#   some resampling could be done, but I will not do that at this point because the number of "positives" may be small and diluted by resampling
#     if resampling is performed on the training set, perhaps use bootstrap to randomly sample (fitControl function)
#   use k-means to cluster and then label the clusters
if(FALSE) {
# wss <- (nrow(features.train.trans)-1)*sum(apply(features.train.trans, 2, var))
# max.clusters <- 20
# for (i in 2:max.clusters) wss[i] <- sum(kmeans(features.train.trans, centers=i)$withinss)
# plot(1:max.clusters, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
# k <- min(which(sapply(2:length(wss), function(x) (wss[x-1] - wss[x])/wss[x-1]) < 0)) - 1
# k.fit <- kmeans(features.train.trans, centers=k, nstart=10, iter.max=100)
# features.train.trans$Label <- k.fit$cluster
# k.labels <- data.frame(Label = seq(1, k, 1), Class = letters[1:k])
# features.train.trans <- merge(features.train.trans, k.labels, by='Label')[,c(2:5)]
} # if the clustering is performed on a training set rather than the whole set
wss <- (nrow(features.pca)-1)*sum(apply(features.pca, 2, var))
max.clusters <- 20
for (i in 2:max.clusters) wss[i] <- sum(kmeans(features.pca, centers=i)$withinss)
plot(1:max.clusters, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
k <- min(which(sapply(2:length(wss), function(x) (wss[x-1] - wss[x])/wss[x-1]) < 0)) - 1
k.fit <- kmeans(features.pca, centers=k, nstart=10, iter.max=100)
features.pca$Label <- k.fit$cluster
k.labels <- data.frame(Label = seq(1, k, 1), Class = letters[1:k])
features.pca.classified <- merge(features.pca, k.labels, by='Label')[,c(2:5)]
#   bind the feature data back to the base data set and generate a signiture for each run
clustered.df <- cbind(base, features.pca.classified)
library(RODBC)
library(lubridate)
library(ggplot2)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
set.seed(3456)
if(FALSE) {
# trainIndex <- createDataPartition(cp.clean$RunDataId, p=0.6, list=FALSE, times=1)
# cp.train <- cp.clean[trainIndex, ]
# cp.test <- cp.clean[-trainIndex, ]
# base.train <- cp.clean[trainIndex, c('RunDataId','Date','YearWeek','CustomerSiteId')]
# features.train <- cp.clean[trainIndex, row.names(keep.vars)]
# base.test <- cp.clean[-trainIndex, c('RunDataId','Date','YearWeek','CustomerSiteId')]
# features.test <- cp.clean[-trainIndex, row.names(keep.vars)]
} # if the data do need to be split into train and test....
base <- cp.clean[, c('RunDataId','Date','YearWeek','CustomerSiteId')]
features <- cp.clean[, row.names(keep.vars)]
#   preprocess the data using PCA option in caret... use the transformation on the training and test sets
preProcValues <- preProcess(features, method = 'pca')
features.pca <- predict(preProcValues, features)
#   some resampling could be done, but I will not do that at this point because the number of "positives" may be small and diluted by resampling
#     if resampling is performed on the training set, perhaps use bootstrap to randomly sample (fitControl function)
#   use k-means to cluster and then label the clusters
if(FALSE) {
# wss <- (nrow(features.train.trans)-1)*sum(apply(features.train.trans, 2, var))
# max.clusters <- 20
# for (i in 2:max.clusters) wss[i] <- sum(kmeans(features.train.trans, centers=i)$withinss)
# plot(1:max.clusters, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
# k <- min(which(sapply(2:length(wss), function(x) (wss[x-1] - wss[x])/wss[x-1]) < 0)) - 1
# k.fit <- kmeans(features.train.trans, centers=k, nstart=10, iter.max=100)
# features.train.trans$Label <- k.fit$cluster
# k.labels <- data.frame(Label = seq(1, k, 1), Class = letters[1:k])
# features.train.trans <- merge(features.train.trans, k.labels, by='Label')[,c(2:5)]
} # if the clustering is performed on a training set rather than the whole set
wss <- (nrow(features.pca)-1)*sum(apply(features.pca, 2, var))
max.clusters <- 20
for (i in 2:max.clusters) wss[i] <- sum(kmeans(features.pca, centers=i)$withinss)
plot(1:max.clusters, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
k <- min(which(sapply(2:length(wss), function(x) (wss[x-1] - wss[x])/wss[x-1]) < 0)) - 1
k.fit <- kmeans(features.pca, centers=k, nstart=10, iter.max=100)
features.pca$Label <- k.fit$cluster
k.labels <- data.frame(Label = seq(1, k, 1), Class = letters[1:k])
features.pca.classified <- merge(features.pca, k.labels, by='Label')[,c(2:5)]
#   bind the feature data back to the base data set and generate a signiture for each run
clustered.df <- cbind(base, features.pca.classified)
clustered.df[clustered.df$RunDataId %in% cp.sequence[grep('^HRV4$|^HRV4, HRV1, HRV2, HRV3$|^HRv4, HRV1, HRV2$|^HRV4, HRV1$', cp.sequence$Sequence), 'RunDataId'], 'SequenceFlag'] <- 'Positive'
clustered.df[is.na(clustered.df$SequenceFlag), 'SequenceFlag'] <- 'Negative'
head(clutered.df)
head(clustered.df)
clustered.df[clustered.df$SequenceFlag=='Positive', ]
clustered.df[clustered.df$SequenceFlag=='Positive', 'Class']
unique(clustered.df[clustered.df$SequenceFlag=='Positive', 'Class'])
unique(as.character(clustered.df[clustered.df$SequenceFlag=='Positive', 'Class']))
hist(as.character(clustered.df[clustered.df$SequenceFlag=='Positive', 'Class']))
hist(clustered.df[clustered.df$SequenceFlag=='Positive', 'Class'])
hist(clustered.df[clustered.df$SequenceFlag=='Positive', 'Class'])
hist(data.frame(Class = clustered.df[clustered.df$SequenceFlag=='Positive', 'Class'], Count = 1)$Count)
unique(as.character(clustered.df[clustered.df$SequenceFlag=='Positive', 'Class']))
clustered.df[clustered.df$SequenceFlag=='Positive', 'Class']
head(clustered.df)
clustered.df$Record <- 1
ggplot(clustered.df, aes(x=Date, y=Record, fill=Class)) + geom_bar(stat='identity')
k.labels
features.pca.classified
View(features.pca.classified)
features.pca$Label <- k.fit$cluster
clustered.df <- cbind(base, features.pca)
clustered.df$SequenceFlag <- NA
clustered.df[clustered.df$RunDataId %in% cp.sequence[grep('^HRV4$|^HRV4, HRV1, HRV2, HRV3$|^HRv4, HRV1, HRV2$|^HRV4, HRV1$', cp.sequence$Sequence), 'RunDataId'], 'SequenceFlag'] <- 'Positive'
clustered.df[is.na(clustered.df$SequenceFlag), 'SequenceFlag'] <- 'Negative'
head(clustered.df)
clustered.df <- merge(clustered.df, k.labels, by='Label')[,c(2:5)]
clustered.df$Record <- 1
ggplot(clustered.df, aes(x=Date, y=Record, fill=Class)) + geom_bar(stat='identity')
head(clustered.df)
head(features.pca)
clustered.df <- cbind(base, features.pca)
clustered.df$SequenceFlag <- NA
clustered.df[clustered.df$RunDataId %in% cp.sequence[grep('^HRV4$|^HRV4, HRV1, HRV2, HRV3$|^HRv4, HRV1, HRV2$|^HRV4, HRV1$', cp.sequence$Sequence), 'RunDataId'], 'SequenceFlag'] <- 'Positive'
clustered.df[is.na(clustered.df$SequenceFlag), 'SequenceFlag'] <- 'Negative'
clustered.df <- merge(clustered.df, k.labels, by='Label')
clustered.df$Record <- 1
head(clustered.df)
ggplot(clustered.df, aes(x=Date, y=Record, fill=Class)) + geom_bar(stat='identity')
clustered.df[clustered.df$SequenceFlag=='Positive', ]
clustered.df[clustered.df$SequenceFlag=='Positive', 'Class']
unique(as.character(clustered.df[clustered.df$SequenceFlag=='Positive', 'Class']))
unique(as.character(clustered.df[clustered.df$SequenceFlag=='Positive', 'Class']))
head(clustered.df)
ggplot(subset(clustered.df, SequenceFlag=='Positive'), aes(x=Date, y=Record, fill=Class)) + geom_bar(stat='identity')
library(RODBC)
library(lubridate)
library(ggplot2)
library(devtools)
require(dateManip)
library(cluster)
library(caret)
library(dbscan)
library(C50)
library(tidyr)
library(dplyr)
head(clustered.df)
ggplot(subset(clustered.df, SequenceFlag=='Positive'), aes(x=Date, y=Record, fill=Class)) + geom_bar(stat='identity')
ggplot(clustered.df, aes(x=Date, y=Record, fill=SequenceFlag)) + geom_bar(stat='identity')
head(clustered.df)
with(clustered.df, aggregate(Record~Class, FUN=sum))
with(with(clustered.df, aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
head(clustered.df)
with(with(subset(clustered.df, year(Date)==2014), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
with(with(subset(clustered.df, year(Date)==2015), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
with(with(subset(clustered.df, year(Date)==2016), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
par(mfrow=c(3,1))
with(with(subset(clustered.df, year(Date)==2014), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
with(with(subset(clustered.df, year(Date)==2015), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
with(with(subset(clustered.df, year(Date)==2016), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
dev.off()
par(mfrow=c(3,1))
with(with(subset(clustered.df, year(Date)==2014), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
with(with(subset(clustered.df, year(Date)==2015), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
with(with(subset(clustered.df, year(Date)==2016), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
dev.off()
par(mfrow=c(3,1))
with(with(subset(clustered.df, year(Date)==2014), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
with(with(subset(clustered.df, year(Date)==2015), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
with(with(subset(clustered.df, year(Date)==2016), aggregate(Record~Class, FUN=sum)), plot(x=Class, y=Record))
head(clustered.df)
with(subset(clustered.df, year(Date)==2014), aggregate(Record~SequenceFlag+Class, FUN=sum))
with(subset(clustered.df, year(Date)==2014 & SequenceFlag=='Positive'), aggregate(Record~Class, FUN=sum))
with(subset(clustered.df, year(Date)==2015 & SequenceFlag=='Positive'), aggregate(Record~Class, FUN=sum))
with(subset(clustered.df, year(Date)==2016 & SequenceFlag=='Positive'), aggregate(Record~Class, FUN=sum))
summary(k.fit)
k.fit$centers
with(subset(clustered.df, year(Date)==2014), aggregate(Record~SequenceFlag+Class, FUN=sum))
head(clusters.df)
head(clustered.df)
head(clustered.df)
a <- with(clustered.df, aggregate(Record~YearWeek+Class, FUN=sum))
b <- with(clustered.df, aggregate(Record~YearWeek, FUN=sum))
head(a)
head(b)
d <- merge(a, b, by='YearWeek')
head(d)
d$Density <- with(d, Record.x/Record.y)
head(d)
ggplot(d, aes(x=YearWeek, y=Density, group=Class, color=Class)) + geom_line(size=2)
ggplot(d, aes(x=YearWeek, y=Density, group=Class, color=Class)) + geom_line(size=1.25)
ggplot(d, aes(x=YearWeek, y=Density, group=Class, color=Class)) + geom_line(size=1.25) + facet_wrap(~Class)
ggplot(d[grep('2014', d$YearWeek), ], aes(x=YearWeek, y=Density, group=Class, color=Class)) + geom_line(size=1.25) + facet_wrap(~Class)
ggplot(d[grep('2015', d$YearWeek), ], aes(x=YearWeek, y=Density, group=Class, color=Class)) + geom_line(size=1.25) + facet_wrap(~Class)
ggplot(d[grep('2016', d$YearWeek), ], aes(x=YearWeek, y=Density, group=Class, color=Class)) + geom_line(size=1.25) + facet_wrap(~Class)
